{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ea4475-d1c4-4e1f-8fd7-7dddb285bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "from datetime import datetime, timedelta, date\n",
    "import pytz\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as clr\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e892ecc-9134-42fc-a817-58016cb189e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"WKU\"\n",
    "data_folder = \"./data\"\n",
    "output_folder = \"./output\"\n",
    "sim_id = 165\n",
    "sim_tz = \"Asia/Shanghai\"\n",
    "time0 = 'Nov 20 2023 9:00AM'\n",
    "time1 = 'Dec 4 2023 12:00PM'\n",
    "time_step_min = 30\n",
    "zenodo_record_id = '10674401'\n",
    "\n",
    "use_new_id_schema = True   \n",
    "\n",
    "# Print warning messages to the console when parsing data\n",
    "print_data_warnings = False\n",
    "\n",
    "discard_reinfections = True\n",
    "\n",
    "# Default contact time for transmissions that are missing an associated contact event\n",
    "def_contact_time = 10\n",
    "\n",
    "# Time delta for plots in seconds\n",
    "time_delta_sec = 60 * time_step_min\n",
    "\n",
    "if not path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# https://howchoo.com/g/ywi5m2vkodk/working-with-datetime-objects-and-timezones-in-python\n",
    "# https://itnext.io/working-with-timezone-and-python-using-pytz-library-4931e61e5152\n",
    "timezone = pytz.timezone(sim_tz)\n",
    "\n",
    "if time0 and time1:\n",
    "    obs_date0 = timezone.localize(datetime.strptime(time0, '%b %d %Y %I:%M%p'))\n",
    "    obs_date1 = timezone.localize(datetime.strptime(time1, '%b %d %Y %I:%M%p'))\n",
    "else:\n",
    "    obs_date0 = None\n",
    "    obs_date1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "347a85a2-f8c0-48dc-a1d1-32f73fa003e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data file ./data/participants.csv\n",
      "Found data file ./data/histories.csv\n",
      "Found data file ./data/survey.csv\n",
      "Found data file ./data/sequences.csv\n",
      "Found data file ./data/mutations.csv\n"
     ]
    }
   ],
   "source": [
    "# Won't work until the record is made public\n",
    "skip_download = True\n",
    "\n",
    "data_files = ['participants.csv', 'histories.csv', 'survey.csv', 'sequences.csv', 'mutations.csv']\n",
    "zenodo_url = 'https://zenodo.org/record/' + zenodo_record_id + '/files/'\n",
    "\n",
    "for fn in data_files:\n",
    "    full_src_path = zenodo_url + fn\n",
    "    dest_path = path.join(data_folder, fn)    \n",
    "    if path.isfile(dest_path):\n",
    "        print('Found data file', dest_path)\n",
    "    elif not skip_download:\n",
    "        print('Downloading', full_src_path, 'to', dest_path, '...')        \n",
    "        wget.download(full_src_path)\n",
    "        print(' Done.')\n",
    "        shutil.move(fn, dest_path)        \n",
    "    else:\n",
    "        print('WARNING: Data file', dest_path, 'is missing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba65004-e7e4-4c35-af2c-b1df57cbd2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parsing functions\n",
    "\n",
    "# It calculates the weights in minutes\n",
    "def get_contact_list(events, infections):\n",
    "    mili_to_seconds = 60 * 1000\n",
    "    \n",
    "    contacts = events[events[\"type\"] == \"contact\"]\n",
    "\n",
    "    node0 = contacts.user_id.values\n",
    "    node1 = contacts.peer_id.values\n",
    "    length = contacts.contact_length.values\n",
    "\n",
    "    clist = {}\n",
    "    for id0, id1, l01 in zip(node0, node1, length):\n",
    "        n0 = user_index[id0]\n",
    "        n1 = -1\n",
    "        if use_new_id_schema:\n",
    "            if id1 in user_index:\n",
    "                n1 = user_index[id1]\n",
    "            elif print_data_warnings:\n",
    "                print(\"Cannot find peer\", id1)\n",
    "        else:\n",
    "            if id1 in p2pToId:\n",
    "                n1 = user_index[p2pToId[id1]]\n",
    "            elif print_data_warnings:\n",
    "                print(\"Cannot find peer\", id1)\n",
    "    \n",
    "        if -1 < n1:\n",
    "            if n0 < n1:\n",
    "                p01 = (n0, n1)\n",
    "            else:\n",
    "                p01 = (n1, n0)\n",
    "            if p01 in clist:\n",
    "                c = clist[p01]\n",
    "            else: \n",
    "                c = 0\n",
    "            \n",
    "            clist[p01] = c + round(l01 / mili_to_seconds)\n",
    "\n",
    "    for p in clist:\n",
    "        clist[p] /= 2\n",
    "    \n",
    "    # Adding contacts from transmissions if they are not registered as contacts already\n",
    "    for (n0, n1) in infections:\n",
    "        if n0 < n1:\n",
    "            p01 = (n0, n1)\n",
    "        else:\n",
    "            p01 = (n1, n0)\n",
    "        if not p01 in clist:\n",
    "            clist[p01] = def_contact_time\n",
    "            if print_data_warnings: print(\"Cannot find contact between\", n0, \"and\", n1)            \n",
    "\n",
    "    return clist\n",
    "\n",
    "def get_infection_list(events):\n",
    "    infections = events[(events[\"type\"] == \"infection\")]\n",
    "    \n",
    "    ilist = []\n",
    "    itimes = {}\n",
    "    infected = infections.user_id.values\n",
    "    peers = infections.inf.values\n",
    "    timestamp = infections.time.values\n",
    "    for id1, peer0, ts in zip(infected, peers, timestamp):\n",
    "        n1 = user_index[id1]\n",
    "            \n",
    "        if \"PEER\" in peer0:\n",
    "            if use_new_id_schema:\n",
    "                # New schema\n",
    "                id0 = int(peer0[peer0.index(\"[\") + 1:peer0.index(\":\")])\n",
    "                if id0 in user_index:\n",
    "                    n0 = user_index[id0]                    \n",
    "                    add_infection = True\n",
    "                    for e in ilist:\n",
    "                        if e[1] == n1:\n",
    "                            if discard_reinfections:\n",
    "                                add_infection = False\n",
    "                                break\n",
    "                            pid0 = index_user[e[0]]\n",
    "                            ts0 = itimes[(pid0, id1)]\n",
    "                            if abs(ts - ts0) <= time_delta_sec:\n",
    "                                add_infection = False\n",
    "                                if print_data_warnings:\n",
    "                                    if pid0 == id0:\n",
    "                                        print(\"Duplicated infection:\", id1, \"was already infected by\", id0, \"in the last\", time_step_min, \"minutes\")\n",
    "                                    else:\n",
    "                                        print(\"Multiple infection:\", id1, \"is being infected by\", id0, \"but was already infected by\", pid0, \"in the last\", time_step_min, \"minutes\")\n",
    "                                break    \n",
    "\n",
    "                    if add_infection: \n",
    "                        ilist += [(n0, n1)]\n",
    "                        itimes[(id0, id1)] = ts\n",
    "                elif print_data_warnings:\n",
    "                    print(\"Cannot find peer\", id0)                    \n",
    "            else:    \n",
    "                # Old schema (sims before 2022): p2p id is in the infection column\n",
    "                p2p0 = peer0[peer0.index(\"[\") + 1:peer0.index(\":\")]\n",
    "                if p2p0 in p2pToId:\n",
    "                    id0 = p2pToId[p2p0]\n",
    "                    if id0 in user_index:\n",
    "                        n0 = user_index[id0]\n",
    "                        if not (n0, n1) in ilist:                        \n",
    "                            ilist += [(n0, n1)]\n",
    "                        elif print_data_warnings:\n",
    "                            print(\"Duplicated infection\", id0, id1)  \n",
    "                elif print_data_warnings:\n",
    "                    print(\"Cannot find peer\", p2p0)                        \n",
    "            \n",
    "    return ilist \n",
    "\n",
    "def get_node_state(events, state0 = None):    \n",
    "    if state0 == None:\n",
    "         state = [0] * len(users)\n",
    "    else:            \n",
    "        state = state0\n",
    "\n",
    "    inf = events[events[\"type\"] == \"infection\"]\n",
    "    infMap = pd.Series(inf.inf.values, index=inf.user_id).to_dict()\n",
    "    for kid in infMap:\n",
    "        src = infMap[kid]\n",
    "        idx = user_index[kid]\n",
    "        if \"CASE0\" in src:\n",
    "            state[idx] = 1\n",
    "        if \"PEER\" in src:\n",
    "            state[idx] = 2\n",
    "            id0 = int(src[5:].split(\":\")[0])\n",
    "            idx0 = user_index[id0]       \n",
    "            if state[idx0] == 0:\n",
    "                state[idx0] = 1\n",
    "                if print_data_warnings:\n",
    "                    print(\"Infecting peer did not have correct state\", idx0)\n",
    "\n",
    "    out = events[events[\"type\"] == \"outcome\"]\n",
    "    outMap = pd.Series(out.out.values, index=out.user_id).to_dict()\n",
    "    for kid in outMap:\n",
    "        out = outMap[kid]\n",
    "        idx = user_index[kid]\n",
    "        if out == \"DEAD\":\n",
    "            state[idx] = 3\n",
    "        if out == \"RECOVERED\":\n",
    "            state[idx] = 4\n",
    "        if out == \"VACCINATED\":\n",
    "            state[idx] = 5\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Some utilities\n",
    "\n",
    "# https://stackoverflow.com/a/48938464\n",
    "def hour_rounder(t):\n",
    "    # Rounds to nearest hour by adding a timedelta hour if minute >= 30\n",
    "    return (t.replace(second=0, microsecond=0, minute=0, hour=t.hour)\n",
    "               +timedelta(hours=t.minute//30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83e8c8c3-925b-48ff-983a-8c258bfe0501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First event: 2023-11-19 15:00:00+08:00\n",
      "Last event : 2023-12-05 16:00:00+08:00\n",
      "Start time: 2023-11-20 09:00:00\n",
      "End time: 2023-12-04 12:00:00\n",
      "Asia/Shanghai\n",
      "794\n",
      "794\n",
      "794\n",
      "794\n"
     ]
    }
   ],
   "source": [
    "# Load participants and histories\n",
    "\n",
    "all_users = pd.read_csv(path.join(data_folder, \"participants.csv\"), low_memory=False) \n",
    "all_events = pd.read_csv(path.join(data_folder, \"histories.csv\"), low_memory=False)\n",
    "\n",
    "users = all_users[all_users[\"sim_id\"] == sim_id]\n",
    "users['random_id'] = users['random_id'].astype(str).str.zfill(4)\n",
    "\n",
    "# Save the users to a pickle file\n",
    "with open(path.join(data_folder, 'users.pickle'), 'wb') as f:\n",
    "    pickle.dump(users, f)\n",
    "\n",
    "events = all_events[all_events[\"sim_id\"] == sim_id]\n",
    "events.fillna({'contact_length':0, 'peer_id':-1}, inplace=True)\n",
    "events[\"event_start\"] = events[\"time\"] - events[\"contact_length\"]/1000\n",
    "events[\"event_start\"] = events[\"event_start\"].astype(int)\n",
    "\n",
    "p2pToSim = pd.Series(users.sim_id.values, index=users.p2p_id).to_dict()\n",
    "p2pToId = pd.Series(users.id.values, index=users.p2p_id).to_dict()\n",
    "idTop2p = pd.Series(users.p2p_id.values, index=users.id).to_dict()\n",
    "        \n",
    "user_index = {}\n",
    "index_user = {}\n",
    "idx = 0\n",
    "for kid in idTop2p:\n",
    "    user_index[kid] = idx\n",
    "    index_user[idx] = kid\n",
    "    idx += 1\n",
    "\n",
    "# Get list of infections and contacts, needed to construct the networkx graph\n",
    "state = get_node_state(events)\n",
    "infections = get_infection_list(events)\n",
    "contacts = get_contact_list(events, infections)\n",
    "\n",
    "# Round min and max times to the hour\n",
    "min_time = min(events['time'])\n",
    "max_time = max(events['time'])\n",
    "first_date = hour_rounder(datetime.fromtimestamp(min_time, tz=timezone))\n",
    "last_date = hour_rounder(datetime.fromtimestamp(max_time, tz=timezone))\n",
    "min_time = datetime.timestamp(first_date)\n",
    "max_time = datetime.timestamp(last_date)\n",
    "\n",
    "print(\"First event:\", first_date)\n",
    "print(\"Last event :\", last_date)\n",
    "\n",
    "if time0 and time1:\n",
    "    print(\"Start time:\", datetime.strptime(time0, '%b %d %Y %I:%M%p'))\n",
    "    print(\"End time:\", datetime.strptime(time1, '%b %d %Y %I:%M%p'))\n",
    "\n",
    "print(first_date.tzinfo)\n",
    "\n",
    "# These should return the same value\n",
    "print(len(users))\n",
    "print(len(idTop2p))    \n",
    "print(len(p2pToId))\n",
    "print(len(user_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83ea7fda-3b82-474e-a0d0-2f1246acf9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794 1802\n",
      "474 1802\n",
      "474 1802\n"
     ]
    }
   ],
   "source": [
    "min_total_contact_time = 5  # at least this total time (in minutes) over the two weeks to be defined as in contact\n",
    "min_total_contact_count = 1 # nodes must have at least this number of edges with other nodes to be kept\n",
    "\n",
    "# Create the network, skipping edges between nodes that spend less than min_contact_time\n",
    "# in contact during the entire sim\n",
    "def create_contact_network(contacts, state, minw=0):\n",
    "    nodes = [i for i in range(0, len(user_index))]\n",
    "    edges = []\n",
    "    weights = []    \n",
    "    if 0 < len(contacts):\n",
    "        for p in contacts:\n",
    "            n0 = p[0]\n",
    "            n1 = p[1]\n",
    "            w = contacts[p]            \n",
    "            if minw < w:\n",
    "                edges += [(n0, n1)]\n",
    "                weights += [w]\n",
    "    \n",
    "    g = nx.Graph()\n",
    "    g.add_nodes_from(nodes)\n",
    "    g.add_weighted_edges_from([(edges[i][0], edges[i][1], weights[i]) for i in range(len(edges))])\n",
    "    \n",
    "    return g\n",
    "\n",
    "def remove_nodes_with_less_edges(G, k):\n",
    "    nodes_to_remove = [node for node, degree in dict(G.degree()).items() if degree < k]\n",
    "    G.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "G = create_contact_network(contacts, state, min_total_contact_time)\n",
    "\n",
    "print(len(G.nodes()), len(G.edges()))\n",
    "\n",
    "remove_nodes_with_less_edges(G, min_total_contact_count)\n",
    "print(len(G.nodes()), len(G.edges()))\n",
    "\n",
    "G.remove_nodes_from(list(nx.isolates(G)))\n",
    "print(len(G.nodes()), len(G.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15f381d5-e64e-48af-8470-e4fe58ab05cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the graph to a file\n",
    "with open(path.join(data_folder, 'full-network.pickle'), 'wb') as f:\n",
    "    pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7b52fa1-ca4b-41ad-928b-20993a11161f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network is connected False\n",
      "472 1801\n",
      "2 1\n"
     ]
    }
   ],
   "source": [
    "# If the Graph has more than one component, this will return False:\n",
    "print(\"Network is connected\", nx.is_connected(G))\n",
    "\n",
    "components = nx.connected_components(G)\n",
    "\n",
    "subgraphs = [G.subgraph(c) for c in components]\n",
    "for sg in subgraphs:\n",
    "    print(len(sg.nodes()), len(sg.edges()))\n",
    "\n",
    "# Calculate the largest connected component subgraph:\n",
    "G = sorted(subgraphs, key=lambda x: len(x))[-1]\n",
    "\n",
    "degrees = [degree for node, degree in G.degree()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f968f1-92f5-41ef-ade6-cc169cff1bec",
   "metadata": {},
   "source": [
    "## Animation of network spread on network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "146c07d9-6a4b-47da-a516-c7c31981f320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the states of each frame...\n",
      "Done\n",
      "Calculated states for 679 frames\n"
     ]
    }
   ],
   "source": [
    "# Generate the state of all nodes in G for each frame of the animation\n",
    "\n",
    "if obs_date0 and obs_date1:\n",
    "    tmin = datetime.timestamp(obs_date0)\n",
    "    tmax = datetime.timestamp(obs_date1)\n",
    "else:\n",
    "    tmin = min_time\n",
    "    tmax = max_time\n",
    "    \n",
    "t = tmin\n",
    "frame = 0\n",
    "all_state = []\n",
    "tstate = None\n",
    "print('Calculating the states of each frame...')\n",
    "while t <= tmax:\n",
    "    t0 = t\n",
    "    t += time_delta_sec\n",
    "    td = datetime.fromtimestamp(t, tz=timezone)\n",
    "    \n",
    "    # We want to include contact and infection events that either started or ended between t0 and t\n",
    "    condition = ((t0 < events['event_start']) & (events['event_start'] <= t)) | ((t0 < events['time']) & (events['time'] <= t))\n",
    "    tevents = events[condition]\n",
    "    tstate = get_node_state(tevents, tstate)\n",
    "\n",
    "    fstate = [tstate[idx] for idx in list(G.nodes())]\n",
    "    all_state.append(fstate)\n",
    "    frame += 1\n",
    "print('Done')\n",
    "\n",
    "num_frames = len(all_state)\n",
    "print(f'Calculated states for {num_frames} frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f298487-7e8e-4f27-9765-c3fef66ca784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the network states to a file\n",
    "with open(path.join(data_folder, 'all-network-states.pickle'), 'wb') as f:\n",
    "    pickle.dump(all_state, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e2b82-c4e6-48a0-bda6-8fd97918819d",
   "metadata": {},
   "source": [
    "## Adding behavioral properties to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28a2334a-3e09-445e-b5d8-1c67763db896",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_survey = pd.read_csv(path.join(data_folder, \"survey.csv\"))\n",
    "\n",
    "# Remove entries with invalid ID\n",
    "user_survey = user_survey[user_survey['user_id'].isin(users['random_id'])]\n",
    "\n",
    "question1 = \"Public health officials should have the power to order people into quarantine during COVID-19 outbreaks\"\n",
    "question2 = \"If someone is given a quarantine order by a public health official, they should follow it no matter what else is going on in their life at work or home\"\n",
    "question3 = \"If I go into quarantine, my family, friends, and community will be protected from getting COVID-19\"\n",
    "\n",
    "demo_vars = ['q1_response', 'q2_response', 'q3_response']\n",
    "action_vars = ['quarantine_yes', 'quarantine_no', 'quarantine_ratio', 'wear_mask', 'num_contacts']\n",
    "attribs = action_vars + demo_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a02eccc5-998a-4ac8-a365-d4f53e3b68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "qy_values = []\n",
    "qn_values = []\n",
    "qr_values = []\n",
    "wm_values = []\n",
    "q1_values = []\n",
    "q2_values = []\n",
    "q3_values = []\n",
    "\n",
    "qy_dict = {}\n",
    "qn_dict = {}\n",
    "qr_dict = {}\n",
    "wm_dict = {}\n",
    "q1_dict = {}\n",
    "q2_dict = {}\n",
    "q3_dict = {}\n",
    "\n",
    "for idx in G.nodes():\n",
    "    uid = users['id'][idx]\n",
    "    rid = users['random_id'][idx]\n",
    "    \n",
    "    user_events = events[events['user_id'] == uid]\n",
    "    qy_ev = user_events[user_events['inf'] == 'quarantine']\n",
    "    qn_ev = user_events[user_events['inf'] == 'noQuarantine']\n",
    "    wm_ev = user_events[user_events['modifier'] == 'Wearing Mask']\n",
    "\n",
    "    qy_num = len(qy_ev)\n",
    "    qn_num = len(qn_ev)\n",
    "    wm_num = len(wm_ev)\n",
    "\n",
    "    if 0 < qy_num + qn_num:\n",
    "        qr_val = qy_num / (qy_num + qn_num)\n",
    "    else:    \n",
    "        qr_val = np.nan\n",
    "    \n",
    "    q1_res = np.nan\n",
    "    q2_res = np.nan\n",
    "    q3_res = np.nan\n",
    "    survey_responses = user_survey[user_survey['user_id'] == rid]\n",
    "    if len(survey_responses) == 1:\n",
    "        q1_res = survey_responses['question1'].values[0]\n",
    "        q2_res = survey_responses['question2'].values[0]\n",
    "        q3_res = survey_responses['question3'].values[0]\n",
    "        \n",
    "    qy_values.append(qy_num)\n",
    "    qn_values.append(qn_num)\n",
    "    qr_values.append(qr_val)\n",
    "    wm_values.append(wm_num)\n",
    "    q1_values.append(q1_res)\n",
    "    q2_values.append(q2_res)\n",
    "    q3_values.append(q3_res)\n",
    "\n",
    "    qy_dict[idx] = qy_num\n",
    "    qn_dict[idx] = qn_num\n",
    "    qr_dict[idx] = qr_val\n",
    "    wm_dict[idx] = wm_num\n",
    "    q1_dict[idx] = q1_res\n",
    "    q2_dict[idx] = q2_res\n",
    "    q3_dict[idx] = q3_res\n",
    "\n",
    "nc_dict = dict(G.degree())\n",
    "\n",
    "user_prefs = pd.DataFrame({'quarantine_yes': qy_values, \n",
    "                           'quarantine_no': qn_values, \n",
    "                           'quarantine_ratio': qr_values, \n",
    "                           'wear_mask': wm_values, \n",
    "                           'q1_response': q1_values, \n",
    "                           'q2_response': q2_values, \n",
    "                           'q3_response': q3_values,\n",
    "                           'num_contacts': degrees})\n",
    "\n",
    "nx.set_node_attributes(G, qy_dict, 'quarantine_yes')\n",
    "nx.set_node_attributes(G, qn_dict, 'quarantine_no')\n",
    "nx.set_node_attributes(G, qr_dict, 'quarantine_ratio')\n",
    "nx.set_node_attributes(G, wm_dict, 'wear_mask')\n",
    "nx.set_node_attributes(G, q1_dict, 'q1_response')\n",
    "nx.set_node_attributes(G, q2_dict, 'q2_response')\n",
    "nx.set_node_attributes(G, q3_dict, 'q3_response')\n",
    "nx.set_node_attributes(G, nc_dict, 'num_contacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6b3bc90-2355-4d36-a965-93b147f1f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.join(data_folder, 'user_prefs.pickle'), 'wb') as f:\n",
    "    pickle.dump(user_prefs, f)\n",
    "    \n",
    "with open(path.join(data_folder, 'network-largest_conn_comp.pickle'), 'wb') as f:\n",
    "    pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46707d68-c806-4c44-92fd-21e7b68cf3eb",
   "metadata": {},
   "source": [
    "## Save transmission tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15ae5f32-558d-4220-bbd5-7a3c72c9e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a new graph using only the transmission (infection) data\n",
    "T = nx.DiGraph(infections)\n",
    "\n",
    "with open(path.join(data_folder, 'transmission-tree.pickle'), 'wb') as f:\n",
    "    pickle.dump(T, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5beec59-9468-44b1-8a1f-df2c84582aa5",
   "metadata": {},
   "source": [
    "## Save daily matrices for factorization analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83d20f40-ef51-4530-928d-d89b3c5fabf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the network for each day of the sim...\n",
      "Frame 1 2023-11-20 09:00 to 2023-11-21 09:00\n",
      "Frame 2 2023-11-21 09:00 to 2023-11-22 09:00\n",
      "Frame 3 2023-11-22 09:00 to 2023-11-23 09:00\n",
      "Frame 4 2023-11-23 09:00 to 2023-11-24 09:00\n",
      "Frame 5 2023-11-24 09:00 to 2023-11-25 09:00\n",
      "Frame 6 2023-11-25 09:00 to 2023-11-26 09:00\n",
      "Frame 7 2023-11-26 09:00 to 2023-11-27 09:00\n",
      "Frame 8 2023-11-27 09:00 to 2023-11-28 09:00\n",
      "Frame 9 2023-11-28 09:00 to 2023-11-29 09:00\n",
      "Frame 10 2023-11-29 09:00 to 2023-11-30 09:00\n",
      "Frame 11 2023-11-30 09:00 to 2023-12-01 09:00\n",
      "Frame 12 2023-12-01 09:00 to 2023-12-02 09:00\n",
      "Frame 13 2023-12-02 09:00 to 2023-12-03 09:00\n",
      "Frame 14 2023-12-03 09:00 to 2023-12-04 09:00\n",
      "Frame 15 2023-12-04 09:00 to 2023-12-05 09:00\n",
      "Done\n",
      "Saved 15 adjacency matrices to a Pickle file.\n"
     ]
    }
   ],
   "source": [
    "# Generate the state of all nodes in G for each frame of the animation\n",
    "\n",
    "if obs_date0 and obs_date1:\n",
    "    tmin = datetime.timestamp(obs_date0)\n",
    "    tmax = datetime.timestamp(obs_date1)\n",
    "else:\n",
    "    tmin = min_time\n",
    "    tmax = max_time\n",
    "\n",
    "# Time delta for plots in seconds\n",
    "daily_delta_sec = 60 * (60 * 24)\n",
    "\n",
    "t = tmin\n",
    "frame = 0\n",
    "\n",
    "tstate = None\n",
    "print('Calculating the network for each day of the sim...')\n",
    "nodes0 = list(G.nodes()) # We only look at the nodes we already selected before (which have enough interactions over the entire period of the sim)\n",
    "allgs = np.zeros((15, len(nodes0), len(nodes0)))\n",
    "while t <= tmax:\n",
    "    t0 = t\n",
    "    t += daily_delta_sec\n",
    "    td = datetime.fromtimestamp(t, tz=timezone)\n",
    "    print('Frame', frame+1, datetime.fromtimestamp(t0, tz=timezone).strftime('%Y-%m-%d %H:%M'), 'to', td.strftime('%Y-%m-%d %H:%M'))\n",
    "    \n",
    "    # We want to include contact and infection events that either started or ended between t0 and t\n",
    "    condition = ((t0 < events['event_start']) & (events['event_start'] <= t)) | ((t0 < events['time']) & (events['time'] <= t))\n",
    "    tevents = events[condition]\n",
    "    tstate = get_node_state(tevents, tstate)\n",
    "    tinf = get_infection_list(tevents)\n",
    "    tcontacts = get_contact_list(tevents, tinf)\n",
    "\n",
    "    tg = nx.Graph()\n",
    "    tg.add_nodes_from(nodes0)\n",
    "    tedges = []\n",
    "    tweights = []\n",
    "    if 0 < len(tcontacts):\n",
    "        for p in tcontacts:\n",
    "            n0 = p[0]\n",
    "            n1 = p[1]\n",
    "            w = tcontacts[p]            \n",
    "            if n0 in nodes0 and n1 in nodes0 and 0 < w:\n",
    "                tedges += [(n0, n1)]\n",
    "                tweights += [w]\n",
    "\n",
    "    tg.add_weighted_edges_from([(tedges[i][0], tedges[i][1], tweights[i]) for i in range(len(tedges))])\n",
    "    adjm = nx.adjacency_matrix(tg).todense()\n",
    "    allgs[frame, :, :] = adjm\n",
    "    \n",
    "    frame += 1\n",
    "print('Done')\n",
    "\n",
    "np.save(path.join(data_folder, 'daily-contact-matrices.npy'), allgs)\n",
    "print(f'Saved {frame} adjacency matrices to a Pickle file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e10f10-14bc-47a4-b535-5627b19fe073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
