{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea53b81-1996-4484-91f4-07b3a0d2d2d8",
   "metadata": {},
   "source": [
    "# Data loading and parsing\n",
    "\n",
    "In this notebook we find load the data from a specific simulation and parse it to be used in the subsequent analysis notebooks.\n",
    "\n",
    "We need to know the simulation ID from the database, the ID of the Zenodo repository where the data has been deposited, and the time zone, start and end times of the simulaton for correctly parsing the data.\n",
    "\n",
    "We use the [pickle module](https://docs.python.org/3/library/pickle.html) to serialize the objects we create in this notebook from the parsed data and save them in binary format to easily load them back into the analysis notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e19315-8de8-40b9-8f52-315106864c2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import pytz\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from scipy.io import mmwrite\n",
    "\n",
    "# Utility code is saved in separate python files to shorten the notebook\n",
    "from utils.parsing import load_properties, hour_rounder\n",
    "from utils.parsing import get_contact_list, get_infection_list, get_node_state, create_contact_network, remove_nodes_with_less_edges\n",
    "from utils.parsing import get_raw_contact_list, get_raw_infection_list, get_raw_outcome_list\n",
    "from utils.parsing import get_all_infection_events, get_all_illness_and_outcome_events\n",
    "from utils.parsing import get_score_events, get_info_count, get_info_users, get_node_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3151e-b287-41d4-b302-5568f2aa59e3",
   "metadata": {},
   "source": [
    "## Input data and settings\n",
    "\n",
    "The input files from the simulation should be stored in the data folder, under a subfolder named as their corresponding ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b41d8-5e35-4a29-ba92-3a2cd7779fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset ID\n",
    "# A few examples: 34 (CMU), 39 (BYU), 165 (WKU), 195 (PSI 2025), 200 (Murdoch 2025), 799 (Bunbury HS)\n",
    "dataset_id = 165\n",
    "\n",
    "# Format of the dataset, options are the follow:\n",
    "# 0 - old OO simulations\n",
    "# 1 - prod OO simulations\n",
    "# 2 - research OO simulations\n",
    "# 3 - epigames\n",
    "data_format = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16047c80-7d6d-409c-a50c-267b0f7be256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for data analysis\n",
    "\n",
    "# Discard transmissions when the infected node was already infected before\n",
    "discard_reinfections = True\n",
    "\n",
    "# Default contact time for transmissions that are missing an associated contact event\n",
    "def_contact_time = 1\n",
    "\n",
    "# Time delta for animation in seconds, needs to be small in comparison with the total length of the \n",
    "# simulation so the changes in the animation are smooth\n",
    "anim_time_step_min = 1\n",
    "anim_time_delta_sec = anim_time_step_min * 60\n",
    "\n",
    "# Time delta for plots in some \"natural\" time step, for examply, hourly or daily\n",
    "nat_time_step_min = 1\n",
    "nat_time_delta_sec = nat_time_step_min * 60\n",
    "\n",
    "min_total_contact_time = 0.16  # at least this total time (in minutes) over the two weeks to be defined as in contact\n",
    "min_total_contact_count = 1 # nodes must have at least this number of edges with other nodes to be kept\n",
    "\n",
    "# Print warning messages to the console when parsing data\n",
    "print_data_warnings = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e892ecc-9134-42fc-a817-58016cb189e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = path.join('./data', str(dataset_id))\n",
    "output_folder = path.join('./output', str(dataset_id))\n",
    "if not path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Load time properties, which need to be in the file time.properties\n",
    "# under the data folder with the following structure:\n",
    "# sim_tz=Europe/London\n",
    "# time0=Jun 30 2025 8:30AM\n",
    "# time1=Jun 30 2025 5:30PM\n",
    "tprops_filename = path.join(data_folder, \"time.properties\")\n",
    "tprops = load_properties(tprops_filename)\n",
    "sim_tz = tprops['sim_tz']\n",
    "time0 = tprops['time0']\n",
    "time1 = tprops['time1']\n",
    "\n",
    "print('Time zone =', sim_tz)\n",
    "print('Start time =', time0)\n",
    "print('End time =', time1)\n",
    "\n",
    "# https://howchoo.com/g/ywi5m2vkodk/working-with-datetime-objects-and-timezones-in-python\n",
    "# https://itnext.io/working-with-timezone-and-python-using-pytz-library-4931e61e5152\n",
    "timezone = pytz.timezone(sim_tz)\n",
    "obs_date0 = timezone.localize(datetime.strptime(time0, '%b %d %Y %I:%M%p'))\n",
    "obs_date1 = timezone.localize(datetime.strptime(time1, '%b %d %Y %I:%M%p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea29495-14ed-40e0-bf67-dc35e9bc4e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if obs_date0 and obs_date1:\n",
    "    tmin = datetime.timestamp(obs_date0)\n",
    "    tmax = datetime.timestamp(obs_date1)\n",
    "else:\n",
    "    tmin = min_time\n",
    "    tmax = max_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f772416c-88cc-44bf-ab6a-e7d2b7d151c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame = 0\n",
    "# t = tmin\n",
    "# frame = 0\n",
    "# while t <= tmax:\n",
    "#     t0 = t\n",
    "#     t += nat_time_delta_sec\n",
    "#     frame += 1\n",
    "# print(frame + 1)\n",
    "\n",
    "num_nat_intervals = int((nat_time_delta_sec + tmax - tmin) / nat_time_delta_sec + 1)\n",
    "num_nat_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e8c8c3-925b-48ff-983a-8c258bfe0501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load participants and histories\n",
    "\n",
    "all_users = pd.read_csv(path.join(data_folder, \"participants.csv\"), low_memory=False) \n",
    "all_events = pd.read_csv(path.join(data_folder, \"histories.csv\"), low_memory=False)\n",
    "    \n",
    "users = all_users[all_users[\"sim_id\"] == dataset_id]\n",
    "if data_format < 2:\n",
    "    if data_format < 1:\n",
    "        users['random_id'] = users['id']\n",
    "else:\n",
    "    users['random_id'] = users['random_id'].astype(str).str.zfill(4)\n",
    "\n",
    "# Save the users to a pickle file\n",
    "with open(path.join(data_folder, 'users.pickle'), 'wb') as f:\n",
    "    pickle.dump(users, f)\n",
    "\n",
    "if data_format == 1:\n",
    "    events = all_events\n",
    "else:\n",
    "    print('whaaa', data_format)\n",
    "    events = all_events[all_events[\"sim_id\"] == dataset_id]\n",
    "    \n",
    "events.fillna({'contact_length':0, 'peer_id':-1}, inplace=True)\n",
    "events[\"event_start\"] = events[\"time\"] - events[\"contact_length\"]/1000\n",
    "events[\"event_start\"] = events[\"event_start\"].astype(int)\n",
    "\n",
    "p2pToSim = pd.Series(users.sim_id.values, index=users.p2p_id).to_dict()\n",
    "p2pToId = pd.Series(users.id.values, index=users.p2p_id).to_dict()\n",
    "idTop2p = pd.Series(users.p2p_id.values, index=users.id).to_dict()\n",
    "        \n",
    "user_index = {}\n",
    "index_user = {}\n",
    "idx = 0\n",
    "for kid in idTop2p:\n",
    "    user_index[kid] = idx\n",
    "    index_user[idx] = kid\n",
    "    idx += 1\n",
    "\n",
    "# Round min and max times to the hour\n",
    "min_time = min(events['time'])\n",
    "max_time = max(events['time'])\n",
    "first_date = hour_rounder(datetime.fromtimestamp(min_time, tz=timezone))\n",
    "last_date = hour_rounder(datetime.fromtimestamp(max_time, tz=timezone))\n",
    "min_time = datetime.timestamp(first_date)\n",
    "max_time = datetime.timestamp(last_date)\n",
    "\n",
    "print(\"First event:\", first_date)\n",
    "print(\"Last event :\", last_date)\n",
    "\n",
    "if time0 and time1:\n",
    "    print(\"Start time:\", datetime.strptime(time0, '%b %d %Y %I:%M%p'))\n",
    "    print(\"End time:\", datetime.strptime(time1, '%b %d %Y %I:%M%p'))\n",
    "\n",
    "print(first_date.tzinfo)\n",
    "\n",
    "# These should return the same value\n",
    "print(len(users))\n",
    "print(len(idTop2p))    \n",
    "print(len(p2pToId))\n",
    "print(len(user_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc3ab94-effd-400e-b49f-b7ce3616b164",
   "metadata": {},
   "source": [
    "At this point, we have parsed the source simulation data and we can use it to extract any information we need from it. For example, in the cell below, we get the final states of all nodes, the list of infections (all the (infectors, infectees) pairs) and the list of all contacts during the entire simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb57d951-7cc7-48bf-ac6a-adccce187c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of infections and contacts, needed to construct the network graph\n",
    "state = get_node_state(user_index, events, None, p2pToId, data_format, print_data_warnings)\n",
    "infections = get_infection_list(user_index, events, discard_reinfections, anim_time_delta_sec, p2pToId, data_format, print_data_warnings)\n",
    "contacts = get_contact_list(user_index, events, infections, def_contact_time, p2pToId, data_format, print_data_warnings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da1da4-a72f-4594-a4e0-2c594234da7d",
   "metadata": {},
   "source": [
    "With the contacts and state information, we can build the network graph using the [networkx package](https://networkx.org/). The first step is to construct the full network where we remove isolated nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea7fda-3b82-474e-a0d0-2f1246acf9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = create_contact_network(user_index, contacts, state, \"final_health_state\", min_total_contact_time)\n",
    "\n",
    "print(len(G.nodes()), len(G.edges()))\n",
    "\n",
    "removed = remove_nodes_with_less_edges(G, min_total_contact_count)\n",
    "\n",
    "isolates = list(nx.isolates(G))\n",
    "G.remove_nodes_from(isolates)\n",
    "\n",
    "mask = users.index.isin(removed + isolates)\n",
    "remids = pd.DataFrame(users[mask]['random_id'].tolist(), columns=['User ID'])\n",
    "remids.to_csv(path.join(data_folder, 'removed-nodes.csv'), index=False)\n",
    "\n",
    "print('Removed', len(remids), 'nodes without enough connections')\n",
    "print('There are', len(G.nodes()), 'remaining nodes with', len(G.edges()), 'edges between them')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f381d5-e64e-48af-8470-e4fe58ab05cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full network into a pickle file for later use\n",
    "with open(path.join(data_folder, 'full-network.pickle'), 'wb') as f:\n",
    "    pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ce8a8f-3b0a-4883-83a0-ad562f7a7166",
   "metadata": {},
   "source": [
    "We also save the directed graph containing all the transmission trees from the simulaton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb70337-3977-46f5-baff-72df87ac6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a new graph using only the transmission (infection) data\n",
    "T = nx.DiGraph(infections)\n",
    "\n",
    "with open(path.join(data_folder, 'transmission-tree.pickle'), 'wb') as f:\n",
    "    pickle.dump(T, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b1e8d-0de8-4e2f-b15e-d1b9af51f6df",
   "metadata": {},
   "source": [
    "## Getting the largest connected subgraph\n",
    "\n",
    "We will conduct the network analyses on the largest connected subgraph in the network, we cand find it using the code in the following cell. We don't save the subgraph yet becasue we will add some properties to the nodes later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b52fa1-ca4b-41ad-928b-20993a11161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the Graph has more than one component, this will return False:\n",
    "print(\"Network is connected\", nx.is_connected(G))\n",
    "\n",
    "components = nx.connected_components(G)\n",
    "\n",
    "subgraphs = [G.subgraph(c) for c in components]\n",
    "for sg in subgraphs:\n",
    "    print(len(sg.nodes()), len(sg.edges()))\n",
    "\n",
    "# Calculate the largest connected component subgraph:\n",
    "G = sorted(subgraphs, key=lambda x: len(x))[-1]\n",
    "\n",
    "degrees = [degree for node, degree in G.degree()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f968f1-92f5-41ef-ade6-cc169cff1bec",
   "metadata": {},
   "source": [
    "## Animation of network spread on network\n",
    "\n",
    "If we save the states of all nodes during the simulation at a give interval, we can then use those states to color the nodes in an animaton that is generated in the network properties notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c07d9-6a4b-47da-a516-c7c31981f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the state of all nodes in G for each frame of the animation\n",
    "    \n",
    "t = tmin\n",
    "frame = 0\n",
    "all_state = []\n",
    "tstate = None\n",
    "print('Calculating the state of each animation frame...')\n",
    "while t <= tmax:\n",
    "    t0 = t\n",
    "    t += anim_time_delta_sec\n",
    "    td = datetime.fromtimestamp(t, tz=timezone)\n",
    "    print(frame, end=' ')\n",
    "    \n",
    "    # We want to include contact and infection events that either started or ended between t0 and t\n",
    "    condition = ((t0 < events['event_start']) & (events['event_start'] <= t)) | ((t0 < events['time']) & (events['time'] <= t))\n",
    "    tevents = events[condition]\n",
    "    tstate = get_node_state(user_index, tevents, tstate, p2pToId, data_format, print_data_warnings)\n",
    "\n",
    "    fstate = [tstate[idx] for idx in list(G.nodes())]\n",
    "    all_state.append(fstate)\n",
    "    frame += 1\n",
    "print('\\nDone')\n",
    "\n",
    "print(f'Calculated states for {len(all_state)} frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f298487-7e8e-4f27-9765-c3fef66ca784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the network states to a file\n",
    "with open(path.join(data_folder, 'all-network-states.pickle'), 'wb') as f:\n",
    "    pickle.dump(all_state, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b3bc90-2355-4d36-a965-93b147f1f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.join(data_folder, 'network-largest_conn_comp.pickle'), 'wb') as f:\n",
    "    pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b2b9d-a549-42af-ad37-6f497d4f907e",
   "metadata": {},
   "source": [
    "## Contact matrices and states per \"natural time interval\" of the simulation\n",
    "\n",
    "Finally, we generate and save the adjacency matrices from the contact network generated for each natural interval of the simulation (only for those nodes present in the full network), as well as the states of each node. This information will be used in the analysis notebooks for tensor factorization and risk prediction.\n",
    "\n",
    "Also, count the \"score\" events from each interval to plot the behaviors such as quarantine, masking, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d20f40-ef51-4530-928d-d89b3c5fabf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the state of all nodes in G for each frame of the animation\n",
    "\n",
    "t = tmin\n",
    "frame = 0\n",
    "\n",
    "tstate = None\n",
    "print('Calculating the network for each frame of the sim...')\n",
    "nodes0 = list(G.nodes()) # We only look at the nodes we already selected before (which have enough interactions over the entire period of the sim)\n",
    "daily_adj = np.zeros((num_nat_intervals, len(nodes0), len(nodes0)))\n",
    "daily_inf = []\n",
    "daily_states = []\n",
    "\n",
    "# Versions of daily_inf and daily_states using indexing reflecting number of nodes after filtering\n",
    "daily_inf_idx0 = []\n",
    "daily_states_idx0 = []\n",
    "# Some notes to explain the need for these additional vars: \n",
    "# In principle, each participant gets a UUID as well as a unique numerical ID, e.g: 9157781f-a054-424b-aa23-4e85ba4e8541 (UUID) \n",
    "# and 10310 (user numerical ID). One can use the dictionaries idTop2p and p2pToId to resolve the mappings between user ID and UUIDs.\n",
    "# Then, each user was assigned a numerical index from 0 to N-1, where N is the number of participants in a particular sim. \n",
    "# So for example, user with ID 10310 is 0, and user with ID 10311 is 1, and so on.\n",
    "# However, as some users can be filtered out for the network analyses if they do not have any contacts, this potentially reduces the\n",
    "# number of nodes in the network down to M < N. This is the size of the networkX object G. The thing is that the nodes G are labelled\n",
    "# with the indices from 0 to N-1, not 0 to M-1, and as it turns out, the values in the daily_states array correspond to the later, \n",
    "# not the former, so you you would need to convert the the values in daily_inf [0 to N-1] to the [0, M-1] range. Fortunately, one can \n",
    "# do this using the nodes0 array, which is simply the list of node labels in the G object. \n",
    "\n",
    "while t <= tmax:\n",
    "    t0 = t\n",
    "    t += nat_time_delta_sec\n",
    "    td = datetime.fromtimestamp(t, tz=timezone)\n",
    "    if num_nat_intervals < 50:\n",
    "        print('Frame', frame+1, datetime.fromtimestamp(t0, tz=timezone).strftime('%Y-%m-%d %H:%M'), 'to', td.strftime('%Y-%m-%d %H:%M'))\n",
    "    \n",
    "    # We want to include contact and infection events that either started or ended between t0 and t\n",
    "    condition = ((t0 < events['event_start']) & (events['event_start'] <= t)) | ((t0 < events['time']) & (events['time'] <= t))\n",
    "    tevents = events[condition]\n",
    "    tstate = get_node_state(user_index, tevents, tstate, p2pToId, data_format, False)\n",
    "    tinf = get_infection_list(user_index, tevents, discard_reinfections, nat_time_delta_sec, p2pToId, data_format, False)\n",
    "    tcontacts = get_contact_list(user_index, tevents, tinf, def_contact_time, p2pToId, data_format, False)\n",
    "    \n",
    "    tg = nx.Graph()\n",
    "    tg.add_nodes_from(nodes0)\n",
    "    tedges = []\n",
    "    tweights = []\n",
    "    if 0 < len(tcontacts):\n",
    "        for p in tcontacts:\n",
    "            n0 = p[0]\n",
    "            n1 = p[1]\n",
    "            w = tcontacts[p]            \n",
    "            if n0 in nodes0 and n1 in nodes0 and 0 < w:\n",
    "                tedges += [(n0, n1)]\n",
    "                tweights += [w]\n",
    "\n",
    "\n",
    "    daily_states.append([tstate[idx] for idx in list(tg.nodes())])    \n",
    "    daily_inf.append(tinf)\n",
    "\n",
    "    # [1,M-1] indexed variables:\n",
    "    tinf_idx0 = []\n",
    "    for i, j in tinf:\n",
    "        tinf_idx0.append((nodes0.index(i), nodes0.index(j)))\n",
    "    daily_inf_idx0.append(tinf_idx0)\n",
    "    daily_states_idx0.append([daily_states[frame][nodes0.index(idx)] for idx in list(tg.nodes())])    \n",
    "    \n",
    "    tg.add_weighted_edges_from([(tedges[i][0], tedges[i][1], tweights[i]) for i in range(len(tedges))])\n",
    "    adjm = nx.adjacency_matrix(tg).todense()\n",
    "    daily_adj[frame, :, :] = adjm\n",
    "    \n",
    "    frame += 1\n",
    "print('Done')\n",
    "num_frames = frame\n",
    "\n",
    "np.save(path.join(data_folder, 'daily-contact-matrices.npy'), daily_adj)\n",
    "print(f'Saved {num_nat_intervals} adjacency matrices to a Pickle file.')\n",
    "\n",
    "with open(path.join(data_folder, 'daily-transmissions.npy'), 'wb') as f:\n",
    "    pickle.dump(daily_inf, f)\n",
    "\n",
    "with open(path.join(data_folder, 'daily-transmissions-idx0.npy'), 'wb') as f:\n",
    "    pickle.dump(daily_inf_idx0, f)\n",
    "\n",
    "print(f'Saved {num_nat_intervals} transmissions lists to a Pickle file.')    \n",
    "\n",
    "with open(path.join(data_folder, 'daily-node-states.pickle'), 'wb') as f:\n",
    "    pickle.dump(daily_states, f)\n",
    "\n",
    "with open(path.join(data_folder, 'daily-node-states-idx0.pickle'), 'wb') as f:\n",
    "    pickle.dump(daily_states_idx0, f)\n",
    "\n",
    "print(f'Saved {num_nat_intervals} states to a Pickle file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b60408-1059-4094-81c6-23f7a44c5822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the contacts (after the filtering before) in raw format (indices, time of formation, duration)\n",
    "\n",
    "raw_contacts = get_raw_contact_list(user_index, events, p2pToId, data_format, print_data_warnings)\n",
    "\n",
    "all_contacts = []\n",
    "for c in raw_contacts:\n",
    "    if c[0] in nodes0 and c[1] in nodes0:\n",
    "        t = int(c[2] - tmin) # time of contact, in seconds\n",
    "        d = int(c[3])        # duration of contact, in miliseconds \n",
    "        if t < tmax - tmin:\n",
    "            cf = (nodes0.index(c[0]), nodes0.index(c[1]), t, d)\n",
    "            all_contacts += [cf]\n",
    "\n",
    "np.save(path.join(data_folder, 'all_contacts.npy'), all_contacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ca32ed-aa8d-45b0-8d07-00313c9ddf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the infections (index cases and person-to-person) in raw format (type, indices, time of infection)\n",
    "\n",
    "raw_infections = get_raw_infection_list(user_index, events, p2pToId, data_format, print_data_warnings)\n",
    "\n",
    "all_infections = []\n",
    "all_infected_nodes = []\n",
    "for i in raw_infections:\n",
    "    t = int(i[3] - tmin)\n",
    "    if t > tmax - tmin:\n",
    "        continue\n",
    "    all_infected_nodes += [i[2]]\n",
    "    if i[0] == 'p2p':  \n",
    "        if i[1] in nodes0 and i[2] in nodes0:\n",
    "            ii = (nodes0.index(i[1]), nodes0.index(i[2]), t)\n",
    "            all_infections += [ii]\n",
    "    elif i[0] == 'idx':\n",
    "        if i[2] in nodes0:\n",
    "            ii = (np.nan, nodes0.index(i[2]), t)\n",
    "            all_infections += [ii]    \n",
    "\n",
    "np.save(path.join(data_folder, 'all_infections.npy'), all_infections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d421865-fc79-4029-948a-32fd6a7e2bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the outcomes in raw format (indices, outcome, time of infection)\n",
    "\n",
    "raw_outcomes = get_raw_outcome_list(user_index, events, print_data_warnings)\n",
    "\n",
    "all_outcomes = []\n",
    "for o in raw_outcomes:\n",
    "    if o[0] in nodes0 and o[0] in all_infected_nodes:\n",
    "        t = int(o[2] - tmin) # time of outcome, in seconds\n",
    "        cf = (nodes0.index(o[0]), o[1], t)\n",
    "        all_outcomes += [cf]\n",
    "\n",
    "np.save(path.join(data_folder, 'all_outcomes.npy'), all_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c089f-0a66-4e0b-aec2-f8d18e651c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029ed192-28f3-4afa-86bc-04dd60639490",
   "metadata": {},
   "source": [
    "## EXTRAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba1b0c-3681-44fd-9088-b1be921e3452",
   "metadata": {},
   "source": [
    "## Saving score-producing events\n",
    "\n",
    "Only applicable for research OO simulations and epigames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce8d14-601f-4884-9e73-b1fe751fa86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "masking = []\n",
    "medication = []\n",
    "quarantine_no = []\n",
    "quarantine_yes = []\n",
    "\n",
    "t = tmin\n",
    "frame = 0\n",
    "print('Calculating score-producing events for each frame of the sim...')\n",
    "if data_format == 1:\n",
    "    print('This does not apply for productin OO simulations')\n",
    "    assert(False)\n",
    "\n",
    "while t <= tmax:\n",
    "    t0 = t\n",
    "    t += nat_time_delta_sec\n",
    "    td = datetime.fromtimestamp(t, tz=timezone)\n",
    "    \n",
    "    # We want to include events that ocurred between t0 and t\n",
    "    condition = (t0 < events['time']) & (events['time'] <= t)\n",
    "\n",
    "    tevents = events[condition]    \n",
    "    score_events = get_score_events(tevents)\n",
    "\n",
    "    date = datetime.fromtimestamp(t0, tz=timezone).strftime('%Y-%m-%d')\n",
    "    mask = get_info_count(score_events, \"shopMask\")\n",
    "    med = get_info_count(score_events, \"shopMedication\")\n",
    "    qno = get_info_count(score_events, \"noQuarantine\")\n",
    "    qyes = get_info_count(score_events, \"quarantine\")\n",
    "\n",
    "    print(date, mask, med, qno, qyes) \n",
    "\n",
    "    dates.append(date)\n",
    "    masking.append(mask)\n",
    "    medication.append(med)\n",
    "    quarantine_no.append(qno)\n",
    "    quarantine_yes.append(qyes)\n",
    "    \n",
    "    frame += 1\n",
    "print('Done')\n",
    "\n",
    "daily_behaviors = pd.DataFrame({'date': dates, \n",
    "                                'masking': masking, \n",
    "                                'medication': medication, \n",
    "                                'quarantine_no': quarantine_no, \n",
    "                                'quarantine_yes': quarantine_yes})\n",
    "\n",
    "with open(path.join(data_folder, 'daily-behaviors.pickle'), 'wb') as f:\n",
    "    pickle.dump(daily_behaviors, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d264f229-e762-4854-974d-c20163a81934",
   "metadata": {},
   "source": [
    "## Converting data to R-compatible format\n",
    "\n",
    "Only needed if planning to conduct difussion analysis in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed6542-ecdf-4486-bf1a-2c05735948cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Exporting the network for each frame of the sim...')\n",
    "\n",
    "tstate = None\n",
    "\n",
    "nodes0 = list(G.nodes()) # We only look at the nodes we already selected before (which have enough interactions over the entire period of the sim)\n",
    "daily_adj = np.zeros((15, len(nodes0), len(nodes0)))\n",
    "daily_inf = []\n",
    "daily_states = []\n",
    "\n",
    "N = len(nodes0)\n",
    "toa = np.array([10000] * N)\n",
    "qyes_list = []\n",
    "qno_list = []\n",
    "mask_list = []\n",
    "med_list = []\n",
    "\n",
    "t = tmin\n",
    "frame = 0\n",
    "while t <= tmax:\n",
    "    t0 = t\n",
    "    t += nat_time_delta_sec\n",
    "    td = datetime.fromtimestamp(t, tz=timezone)\n",
    "    print('Frame', frame+1, datetime.fromtimestamp(t0, tz=timezone).strftime('%Y-%m-%d %H:%M'), 'to', td.strftime('%Y-%m-%d %H:%M'))\n",
    "    \n",
    "    # We want to include contact and infection events that either started or ended between t0 and t\n",
    "    condition = ((t0 < events['event_start']) & (events['event_start'] <= t)) | ((t0 < events['time']) & (events['time'] <= t))\n",
    "    tevents = events[condition]\n",
    "    tstate = get_node_state(user_index, tevents, tstate, p2pToId, data_format, False)\n",
    "    tinf = get_infection_list(user_index, tevents, discard_reinfections, nat_time_delta_sec, p2pToId, data_format, False)\n",
    "    tcontacts = get_contact_list(user_index, tevents, tinf, def_contact_time, p2pToId, data_format, False)\n",
    "    score_events = get_score_events(tevents)\n",
    "    \n",
    "    tg = nx.Graph()\n",
    "    tg.add_nodes_from(nodes0)\n",
    "    tedges = []\n",
    "    tweights = []\n",
    "    if 0 < len(tcontacts):\n",
    "        for p in tcontacts:\n",
    "            n0 = p[0]\n",
    "            n1 = p[1]\n",
    "            w = tcontacts[p]            \n",
    "            if n0 in nodes0 and n1 in nodes0 and 0 < w:\n",
    "                tedges += [(n0, n1)]\n",
    "                tweights += [w]\n",
    "\n",
    "    tg.add_weighted_edges_from([(tedges[i][0], tedges[i][1], tweights[i]) for i in range(len(tedges))])\n",
    "    m = nx.to_scipy_sparse_array(tg, format=\"csr\")\n",
    "    mmwrite(f\"{data_folder}/network_{frame+1}.mtx\", m)\n",
    "    \n",
    "    fstate = np.array([tstate[idx] for idx in list(tg.nodes())])\n",
    "    inf_id = np.where((fstate == 1) | (fstate == 2))[0]\n",
    "    mask = frame + 1 < toa[inf_id]\n",
    "    toa[inf_id[mask]] = frame + 1\n",
    "\n",
    "    qyes = np.zeros(N)    \n",
    "    users_q = get_info_users(score_events, \"quarantine\")    \n",
    "    nidx = get_node_index(users_q, user_index, nodes0)\n",
    "    qyes[nidx] = 1\n",
    "    qyes_list.append(qyes)\n",
    "    \n",
    "    qno = np.zeros(N)\n",
    "    users_nq = get_info_users(score_events, \"noQuarantine\")\n",
    "    nidx = get_node_index(users_nq, user_index, nodes0)\n",
    "    qno[nidx] = 1\n",
    "    qno_list.append(qno)\n",
    "\n",
    "    mask = np.zeros(N)\n",
    "    users_mk = get_info_users(score_events, \"shopMask\")\n",
    "    nidx = get_node_index(users_mk, user_index, nodes0)\n",
    "    mask[nidx] = 1 \n",
    "    mask_list.append(mask)\n",
    "\n",
    "    med = np.zeros(N)\n",
    "    users_md = get_info_users(score_events, \"shopMedication\")\n",
    "    nidx = get_node_index(users_md, user_index, nodes0)\n",
    "    med[nidx] = 1\n",
    "    med_list.append(med)\n",
    "    \n",
    "    frame += 1\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc1dea8-c3f3-4b6d-9fd1-f2c4b9590a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qyes_all = np.concatenate(qyes_list)\n",
    "qno_all = np.concatenate(qno_list)\n",
    "mask_all = np.concatenate(mask_list)\n",
    "med_all = np.concatenate(med_list)\n",
    "\n",
    "toa = toa.astype(float)\n",
    "toa[toa == 10000] = np.nan\n",
    "toa_all = np.tile(toa, num_frames)\n",
    "\n",
    "nodes_all = np.tile(np.arange(N) + 1, num_frames)\n",
    "\n",
    "t_all = np.repeat(np.arange(1, num_frames + 1), N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb0d64-86bc-4811-bab9-fb6e1012f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dat = pd.DataFrame({'idnum': nodes_all, 'qyes': qyes_all, 'qno': qno_all, 'mask': mask_all, 'med': med_all, 'tinf': toa_all, 'time': t_all})\n",
    "net_dat['tinf'] = net_dat['tinf'].astype('Int64')\n",
    "net_dat.to_csv(f\"{data_folder}/network_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a014c50-59a1-4a13-867e-a5fd1ea0a239",
   "metadata": {},
   "source": [
    "## Exporting informaton in simple format\n",
    "\n",
    "This could be useful to create a simple dataset for epi analysis (case counts, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c52619-487f-47b4-8eaf-7996a02090c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_data = get_all_infection_events(events, users, tmin, data_format)\n",
    "inf_data.to_csv(path.join(data_folder, 'list-of-infections.csv'), index=False)\n",
    "\n",
    "inf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7f9c5-7e9c-48d8-9f71-f8cf0c21cf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data = get_all_illness_and_outcome_events(events, users, tmin, data_format)\n",
    "out_data.to_csv(path.join(data_folder, 'list-of-illnesses-and-outcomes.csv'), index=False)\n",
    "\n",
    "out_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab5542-3713-4d64-91c1-8c9430409528",
   "metadata": {},
   "source": [
    "## Exporting informaton of infected nodes\n",
    "\n",
    "Used during PSI conference to generate input data for GLEAMviz sim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40714150-a444-4994-9f42-8a14586df33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tinf_step_min = 60\n",
    "tinf_delta_sec = 60 * tinf_step_min\n",
    "\n",
    "max_frame = 4\n",
    "\n",
    "t = tmin\n",
    "frame = 0\n",
    "\n",
    "cases = []\n",
    "\n",
    "tstate = None\n",
    "print('Calculating transmissions and case info...')\n",
    "nodes0 = list(G.nodes()) # We only look at the nodes we already selected before (which have enough interactions over the entire period of the sim)\n",
    "daily_adj = np.zeros((num_frames, len(nodes0), len(nodes0)))\n",
    "daily_inf = []\n",
    "daily_states = []\n",
    "while t <= tmax and frame < max_frame:\n",
    "    t0 = t\n",
    "    t += tinf_delta_sec\n",
    "    td = datetime.fromtimestamp(t, tz=timezone)\n",
    "    print('Frame', frame+1, datetime.fromtimestamp(t0, tz=timezone).strftime('%Y-%m-%d %H:%M'), 'to', td.strftime('%Y-%m-%d %H:%M'))\n",
    "    \n",
    "    # We want to include contact and infection events that either started or ended between t0 and t\n",
    "    condition = ((t0 < events['event_start']) & (events['event_start'] <= t)) | ((t0 < events['time']) & (events['time'] <= t))\n",
    "    tevents = events[condition]\n",
    "    tstate = get_node_state(user_index, tevents, tstate, False)\n",
    "    tinf = get_infection_list(user_index, tevents, discard_reinfections, tinf_delta_sec)\n",
    "\n",
    "    for pair in tinf:\n",
    "        cases.append(index_user[pair[1]])\n",
    "\n",
    "    frame += 1\n",
    "print('Done')\n",
    "\n",
    "countries = all_events[all_events['type'] == 'modifier']\n",
    "inf_ids = []\n",
    "inf_countries = []\n",
    "for cid in cases:\n",
    "    sel = countries[countries['user_id'] == cid]\n",
    "    country = sel['modifier'].values[0].split(':')[1]\n",
    "    inf_ids.append(cid)\n",
    "    inf_countries.append(country)\n",
    "\n",
    "inf_data = pd.DataFrame({\n",
    "    \"ID\": inf_ids,\n",
    "    \"Country\": inf_countries,\n",
    "    \"State\": ['infectious'] * len(inf_ids)\n",
    "})\n",
    "\n",
    "print(inf_data)\n",
    "\n",
    "inf_data.to_csv(path.join(output_folder, 'gleam-input.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
