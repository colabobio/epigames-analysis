{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea53b81-1996-4484-91f4-07b3a0d2d2d8",
   "metadata": {},
   "source": [
    "# Data loading and parsing\n",
    "\n",
    "In this notebook we find load the data from a specific simulation and parse it to be used in the subsequent analysis notebooks.\n",
    "\n",
    "We need to know the simulation ID from the database, the ID of the Zenodo repository where the data has been deposited, and the time zone, start and end times of the simulaton for correctly parsing the data.\n",
    "\n",
    "We use the [pickle module](https://docs.python.org/3/library/pickle.html) to serialize the objects we create in this notebook from the parsed data and save them in binary format to easily load them back into the analysis notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15e19315-8de8-40b9-8f52-315106864c2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import pytz\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from scipy.io import mmwrite\n",
    "\n",
    "# Utility code is saved in separate python files to shorten the notebook\n",
    "from utils.parsing import get_contact_list, get_infection_list, get_node_state, hour_rounder, create_contact_network, remove_nodes_with_less_edges\n",
    "from utils.parsing import get_score_events, get_info_count, get_info_users, get_node_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3151e-b287-41d4-b302-5568f2aa59e3",
   "metadata": {},
   "source": [
    "## Input data and settings\n",
    "\n",
    "The input files from the simulation should be stored in the data folder, under a subfolder named as their corresponding ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b06b41d8-5e35-4a29-ba92-3a2cd7779fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset ID\n",
    "# 165 (WKU), 195 (PSI 2025), 200 (Murdoch 2025)\n",
    "dataset_id = 200\n",
    "\n",
    "# Format of the dataset, options are the follow:\n",
    "# 0 - old OO simulations\n",
    "# 1 - prod OO simulations\n",
    "# 2 - research OO simulations\n",
    "# 3 - epigames\n",
    "data_format = 2\n",
    "\n",
    "# Time zone, starting and ending time\n",
    "# sim_tz = \"US/Mountain\"\n",
    "# time0 = 'Oct 29 2020 9:00AM'\n",
    "# time1 = 'Nov 4 2020 12:00PM'\n",
    "\n",
    "# Time zone, starting and ending time\n",
    "# sim_tz = \"US/Mountain\"\n",
    "# time0 = 'Feb 19 2021 9:00AM'\n",
    "# time1 = 'Mar 1 2021 12:00PM'\n",
    "\n",
    "# sim_tz = \"Asia/Shanghai\"\n",
    "# time0 = 'Nov 20 2023 9:00AM'\n",
    "# time1 = 'Dec 4 2023 12:00PM'\n",
    "\n",
    "sim_tz = \"Europe/London\"\n",
    "time0 = 'Jun 30 2025 8:30AM'\n",
    "time1 = 'Jun 30 2025 5:30PM'\n",
    "\n",
    "# sim_tz = \"Australia/Perth\"\n",
    "# time0 = 'Oct 7 2025 2:35PM'\n",
    "# time1 = 'Oct 7 2025 4:05PM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "16047c80-7d6d-409c-a50c-267b0f7be256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for data analysis\n",
    "\n",
    "# Discard transmissions when the infected node was already infected before\n",
    "discard_reinfections = True\n",
    "\n",
    "# Default contact time for transmissions that are missing an associated contact event\n",
    "def_contact_time = 10\n",
    "\n",
    "# Time delta for animation in seconds, needs to be small in comparison with the total length of the \n",
    "# simulation so the changes in the animation are smooth\n",
    "anim_time_step_min = 5\n",
    "anim_time_delta_sec = anim_time_step_min * 60\n",
    "\n",
    "# Time delta for plots in some \"natural\" time step, for examply, hourly or daily\n",
    "nat_time_step_min = 5\n",
    "nat_time_delta_sec = nat_time_step_min * 60\n",
    "\n",
    "min_total_contact_time = 1  # at least this total time (in minutes) over the two weeks to be defined as in contact\n",
    "min_total_contact_count = 1 # nodes must have at least this number of edges with other nodes to be kept\n",
    "\n",
    "# Print warning messages to the console when parsing data\n",
    "print_data_warnings = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6e892ecc-9134-42fc-a817-58016cb189e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = path.join('./data', str(dataset_id))\n",
    "output_folder = path.join('./output', str(dataset_id))\n",
    "if not path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# https://howchoo.com/g/ywi5m2vkodk/working-with-datetime-objects-and-timezones-in-python\n",
    "# https://itnext.io/working-with-timezone-and-python-using-pytz-library-4931e61e5152\n",
    "timezone = pytz.timezone(sim_tz)\n",
    "obs_date0 = timezone.localize(datetime.strptime(time0, '%b %d %Y %I:%M%p'))\n",
    "obs_date1 = timezone.localize(datetime.strptime(time1, '%b %d %Y %I:%M%p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1ea29495-14ed-40e0-bf67-dc35e9bc4e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if obs_date0 and obs_date1:\n",
    "    tmin = datetime.timestamp(obs_date0)\n",
    "    tmax = datetime.timestamp(obs_date1)\n",
    "else:\n",
    "    tmin = min_time\n",
    "    tmax = max_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f772416c-88cc-44bf-ab6a-e7d2b7d151c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frame = 0\n",
    "# t = tmin\n",
    "# frame = 0\n",
    "# while t <= tmax:\n",
    "#     t0 = t\n",
    "#     t += nat_time_delta_sec\n",
    "#     frame += 1\n",
    "# print(frame + 1)\n",
    "\n",
    "num_nat_intervals = int((nat_time_delta_sec + tmax - tmin) / nat_time_delta_sec + 1)\n",
    "num_nat_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "83e8c8c3-925b-48ff-983a-8c258bfe0501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First event: 2025-10-07 07:00:00+01:00\n",
      "Last event : 2025-10-07 11:00:00+01:00\n",
      "Start time: 2025-06-30 08:30:00\n",
      "End time: 2025-06-30 17:30:00\n",
      "Europe/London\n",
      "91\n",
      "91\n",
      "91\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "# Load participants and histories\n",
    "\n",
    "all_users = pd.read_csv(path.join(data_folder, \"participants.csv\"), low_memory=False) \n",
    "all_events = pd.read_csv(path.join(data_folder, \"histories.csv\"), low_memory=False)\n",
    "\n",
    "users = all_users[all_users[\"sim_id\"] == dataset_id]\n",
    "if data_format < 2:\n",
    "    users['random_id'] = users['id']\n",
    "else:    \n",
    "    users['random_id'] = users['random_id'].astype(str).str.zfill(4)\n",
    "\n",
    "# Save the users to a pickle file\n",
    "with open(path.join(data_folder, 'users.pickle'), 'wb') as f:\n",
    "    pickle.dump(users, f)\n",
    "\n",
    "events = all_events[all_events[\"sim_id\"] == dataset_id]\n",
    "events.fillna({'contact_length':0, 'peer_id':-1}, inplace=True)\n",
    "events[\"event_start\"] = events[\"time\"] - events[\"contact_length\"]/1000\n",
    "events[\"event_start\"] = events[\"event_start\"].astype(int)\n",
    "\n",
    "p2pToSim = pd.Series(users.sim_id.values, index=users.p2p_id).to_dict()\n",
    "p2pToId = pd.Series(users.id.values, index=users.p2p_id).to_dict()\n",
    "idTop2p = pd.Series(users.p2p_id.values, index=users.id).to_dict()\n",
    "        \n",
    "user_index = {}\n",
    "index_user = {}\n",
    "idx = 0\n",
    "for kid in idTop2p:\n",
    "    user_index[kid] = idx\n",
    "    index_user[idx] = kid\n",
    "    idx += 1\n",
    "\n",
    "# Round min and max times to the hour\n",
    "min_time = min(events['time'])\n",
    "max_time = max(events['time'])\n",
    "first_date = hour_rounder(datetime.fromtimestamp(min_time, tz=timezone))\n",
    "last_date = hour_rounder(datetime.fromtimestamp(max_time, tz=timezone))\n",
    "min_time = datetime.timestamp(first_date)\n",
    "max_time = datetime.timestamp(last_date)\n",
    "\n",
    "print(\"First event:\", first_date)\n",
    "print(\"Last event :\", last_date)\n",
    "\n",
    "if time0 and time1:\n",
    "    print(\"Start time:\", datetime.strptime(time0, '%b %d %Y %I:%M%p'))\n",
    "    print(\"End time:\", datetime.strptime(time1, '%b %d %Y %I:%M%p'))\n",
    "\n",
    "print(first_date.tzinfo)\n",
    "\n",
    "# These should return the same value\n",
    "print(len(users))\n",
    "print(len(idTop2p))    \n",
    "print(len(p2pToId))\n",
    "print(len(user_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc3ab94-effd-400e-b49f-b7ce3616b164",
   "metadata": {},
   "source": [
    "At this point, we have parsed the source simulation data and we can use it to extract any information we need from it. For example, in the cell below, we get the final states of all nodes, the list of infections (all the (infectors, infectees) pairs) and the list of all contacts during the entire simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cb57d951-7cc7-48bf-ac6a-adccce187c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of infections and contacts, needed to construct the network graph\n",
    "state = get_node_state(user_index, events, None, p2pToId, data_format, print_data_warnings)\n",
    "infections = get_infection_list(user_index, events, discard_reinfections, anim_time_delta_sec, p2pToId, data_format, print_data_warnings)\n",
    "contacts = get_contact_list(user_index, events, infections, def_contact_time, p2pToId, data_format, print_data_warnings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da1da4-a72f-4594-a4e0-2c594234da7d",
   "metadata": {},
   "source": [
    "With the contacts and state information, we can build the network graph using the [networkx package](https://networkx.org/). The first step is to construct the full network where we remove isolated nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "83ea7fda-3b82-474e-a0d0-2f1246acf9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 511\n",
      "Removed 3 nodes without enough connections\n",
      "There are 88 remaining nodes with 511 edges between them\n"
     ]
    }
   ],
   "source": [
    "G = create_contact_network(user_index, contacts, state, \"final_health_state\", min_total_contact_time)\n",
    "\n",
    "print(len(G.nodes()), len(G.edges()))\n",
    "\n",
    "removed = remove_nodes_with_less_edges(G, min_total_contact_count)\n",
    "\n",
    "isolates = list(nx.isolates(G))\n",
    "G.remove_nodes_from(isolates)\n",
    "\n",
    "mask = users.index.isin(removed + isolates)\n",
    "remids = pd.DataFrame(users[mask]['random_id'].tolist(), columns=['User ID'])\n",
    "remids.to_csv(path.join(data_folder, 'removed-nodes.csv'), index=False)\n",
    "\n",
    "print('Removed', len(remids), 'nodes without enough connections')\n",
    "print('There are', len(G.nodes()), 'remaining nodes with', len(G.edges()), 'edges between them')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "15f381d5-e64e-48af-8470-e4fe58ab05cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full network into a pickle file for later use\n",
    "with open(path.join(data_folder, 'full-network.pickle'), 'wb') as f:\n",
    "    pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ce8a8f-3b0a-4883-83a0-ad562f7a7166",
   "metadata": {},
   "source": [
    "We also save the directed graph containing all the transmission trees from the simulaton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4eb70337-3977-46f5-baff-72df87ac6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a new graph using only the transmission (infection) data\n",
    "T = nx.DiGraph(infections)\n",
    "\n",
    "with open(path.join(data_folder, 'transmission-tree.pickle'), 'wb') as f:\n",
    "    pickle.dump(T, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b1e8d-0de8-4e2f-b15e-d1b9af51f6df",
   "metadata": {},
   "source": [
    "## Getting the largest connected subgraph\n",
    "\n",
    "We will conduct the network analyses on the largest connected subgraph in the network, we cand find it using the code in the following cell. We don't save the subgraph yet becasue we will add some properties to the nodes later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d7b52fa1-ca4b-41ad-928b-20993a11161f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network is connected True\n",
      "88 511\n"
     ]
    }
   ],
   "source": [
    "# If the Graph has more than one component, this will return False:\n",
    "print(\"Network is connected\", nx.is_connected(G))\n",
    "\n",
    "components = nx.connected_components(G)\n",
    "\n",
    "subgraphs = [G.subgraph(c) for c in components]\n",
    "for sg in subgraphs:\n",
    "    print(len(sg.nodes()), len(sg.edges()))\n",
    "\n",
    "# Calculate the largest connected component subgraph:\n",
    "G = sorted(subgraphs, key=lambda x: len(x))[-1]\n",
    "\n",
    "degrees = [degree for node, degree in G.degree()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f968f1-92f5-41ef-ade6-cc169cff1bec",
   "metadata": {},
   "source": [
    "## Animation of network spread on network\n",
    "\n",
    "If we save the states of all nodes during the simulation at a give interval, we can then use those states to color the nodes in an animaton that is generated in the network properties notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "146c07d9-6a4b-47da-a516-c7c31981f320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the state of each animation frame...\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 \n",
      "Done\n",
      "Calculated states for 109 frames\n"
     ]
    }
   ],
   "source": [
    "# Generate the state of all nodes in G for each frame of the animation\n",
    "    \n",
    "t = tmin\n",
    "frame = 0\n",
    "all_state = []\n",
    "tstate = None\n",
    "print('Calculating the state of each animation frame...')\n",
    "while t <= tmax:\n",
    "    t0 = t\n",
    "    t += anim_time_delta_sec\n",
    "    td = datetime.fromtimestamp(t, tz=timezone)\n",
    "    print(frame, end=' ')\n",
    "    \n",
    "    # We want to include contact and infection events that either started or ended between t0 and t\n",
    "    condition = ((t0 < events['event_start']) & (events['event_start'] <= t)) | ((t0 < events['time']) & (events['time'] <= t))\n",
    "    tevents = events[condition]\n",
    "    tstate = get_node_state(user_index, tevents, tstate, p2pToId, data_format, print_data_warnings)\n",
    "\n",
    "    fstate = [tstate[idx] for idx in list(G.nodes())]\n",
    "    all_state.append(fstate)\n",
    "    frame += 1\n",
    "print('\\nDone')\n",
    "\n",
    "print(f'Calculated states for {len(all_state)} frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0f298487-7e8e-4f27-9765-c3fef66ca784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the network states to a file\n",
    "with open(path.join(data_folder, 'all-network-states.pickle'), 'wb') as f:\n",
    "    pickle.dump(all_state, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f6b3bc90-2355-4d36-a965-93b147f1f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.join(data_folder, 'network-largest_conn_comp.pickle'), 'wb') as f:\n",
    "    pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b2b9d-a549-42af-ad37-6f497d4f907e",
   "metadata": {},
   "source": [
    "## Contact matrices and states per \"natural time interval\" of the simulation\n",
    "\n",
    "Finally, we generate and save the adjacency matrices from the contact network generated for each natural interval of the simulation (only for those nodes present in the full network), as well as the states of each node. This information will be used in the analysis notebooks for tensor factorization and risk prediction.\n",
    "\n",
    "Also, count the \"score\" events from each interval to plot the behaviors such as quarantine, masking, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "83d20f40-ef51-4530-928d-d89b3c5fabf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the network for each frame of the sim...\n",
      "Frame 1 2025-06-30 08:30 to 2025-06-30 08:35\n",
      "Frame 2 2025-06-30 08:35 to 2025-06-30 08:40\n",
      "Frame 3 2025-06-30 08:40 to 2025-06-30 08:45\n",
      "Frame 4 2025-06-30 08:45 to 2025-06-30 08:50\n",
      "Frame 5 2025-06-30 08:50 to 2025-06-30 08:55\n",
      "Frame 6 2025-06-30 08:55 to 2025-06-30 09:00\n",
      "Frame 7 2025-06-30 09:00 to 2025-06-30 09:05\n",
      "Frame 8 2025-06-30 09:05 to 2025-06-30 09:10\n",
      "Frame 9 2025-06-30 09:10 to 2025-06-30 09:15\n",
      "Frame 10 2025-06-30 09:15 to 2025-06-30 09:20\n",
      "Frame 11 2025-06-30 09:20 to 2025-06-30 09:25\n",
      "Frame 12 2025-06-30 09:25 to 2025-06-30 09:30\n",
      "Frame 13 2025-06-30 09:30 to 2025-06-30 09:35\n",
      "Frame 14 2025-06-30 09:35 to 2025-06-30 09:40\n",
      "Frame 15 2025-06-30 09:40 to 2025-06-30 09:45\n",
      "Frame 16 2025-06-30 09:45 to 2025-06-30 09:50\n",
      "Frame 17 2025-06-30 09:50 to 2025-06-30 09:55\n",
      "Frame 18 2025-06-30 09:55 to 2025-06-30 10:00\n",
      "Frame 19 2025-06-30 10:00 to 2025-06-30 10:05\n",
      "Frame 20 2025-06-30 10:05 to 2025-06-30 10:10\n",
      "Frame 21 2025-06-30 10:10 to 2025-06-30 10:15\n",
      "Frame 22 2025-06-30 10:15 to 2025-06-30 10:20\n",
      "Frame 23 2025-06-30 10:20 to 2025-06-30 10:25\n",
      "Frame 24 2025-06-30 10:25 to 2025-06-30 10:30\n",
      "Frame 25 2025-06-30 10:30 to 2025-06-30 10:35\n",
      "Frame 26 2025-06-30 10:35 to 2025-06-30 10:40\n",
      "Frame 27 2025-06-30 10:40 to 2025-06-30 10:45\n",
      "Frame 28 2025-06-30 10:45 to 2025-06-30 10:50\n",
      "Frame 29 2025-06-30 10:50 to 2025-06-30 10:55\n",
      "Frame 30 2025-06-30 10:55 to 2025-06-30 11:00\n",
      "Frame 31 2025-06-30 11:00 to 2025-06-30 11:05\n",
      "Frame 32 2025-06-30 11:05 to 2025-06-30 11:10\n",
      "Frame 33 2025-06-30 11:10 to 2025-06-30 11:15\n",
      "Frame 34 2025-06-30 11:15 to 2025-06-30 11:20\n",
      "Frame 35 2025-06-30 11:20 to 2025-06-30 11:25\n",
      "Frame 36 2025-06-30 11:25 to 2025-06-30 11:30\n",
      "Frame 37 2025-06-30 11:30 to 2025-06-30 11:35\n",
      "Frame 38 2025-06-30 11:35 to 2025-06-30 11:40\n",
      "Frame 39 2025-06-30 11:40 to 2025-06-30 11:45\n",
      "Frame 40 2025-06-30 11:45 to 2025-06-30 11:50\n",
      "Frame 41 2025-06-30 11:50 to 2025-06-30 11:55\n",
      "Frame 42 2025-06-30 11:55 to 2025-06-30 12:00\n",
      "Frame 43 2025-06-30 12:00 to 2025-06-30 12:05\n",
      "Frame 44 2025-06-30 12:05 to 2025-06-30 12:10\n",
      "Frame 45 2025-06-30 12:10 to 2025-06-30 12:15\n",
      "Frame 46 2025-06-30 12:15 to 2025-06-30 12:20\n",
      "Frame 47 2025-06-30 12:20 to 2025-06-30 12:25\n",
      "Frame 48 2025-06-30 12:25 to 2025-06-30 12:30\n",
      "Frame 49 2025-06-30 12:30 to 2025-06-30 12:35\n",
      "Frame 50 2025-06-30 12:35 to 2025-06-30 12:40\n",
      "Frame 51 2025-06-30 12:40 to 2025-06-30 12:45\n",
      "Frame 52 2025-06-30 12:45 to 2025-06-30 12:50\n",
      "Frame 53 2025-06-30 12:50 to 2025-06-30 12:55\n",
      "Frame 54 2025-06-30 12:55 to 2025-06-30 13:00\n",
      "Frame 55 2025-06-30 13:00 to 2025-06-30 13:05\n",
      "Frame 56 2025-06-30 13:05 to 2025-06-30 13:10\n",
      "Frame 57 2025-06-30 13:10 to 2025-06-30 13:15\n",
      "Frame 58 2025-06-30 13:15 to 2025-06-30 13:20\n",
      "Frame 59 2025-06-30 13:20 to 2025-06-30 13:25\n",
      "Frame 60 2025-06-30 13:25 to 2025-06-30 13:30\n",
      "Frame 61 2025-06-30 13:30 to 2025-06-30 13:35\n",
      "Frame 62 2025-06-30 13:35 to 2025-06-30 13:40\n",
      "Frame 63 2025-06-30 13:40 to 2025-06-30 13:45\n",
      "Frame 64 2025-06-30 13:45 to 2025-06-30 13:50\n",
      "Frame 65 2025-06-30 13:50 to 2025-06-30 13:55\n",
      "Frame 66 2025-06-30 13:55 to 2025-06-30 14:00\n",
      "Frame 67 2025-06-30 14:00 to 2025-06-30 14:05\n",
      "Frame 68 2025-06-30 14:05 to 2025-06-30 14:10\n",
      "Frame 69 2025-06-30 14:10 to 2025-06-30 14:15\n",
      "Frame 70 2025-06-30 14:15 to 2025-06-30 14:20\n",
      "Frame 71 2025-06-30 14:20 to 2025-06-30 14:25\n",
      "Frame 72 2025-06-30 14:25 to 2025-06-30 14:30\n",
      "Frame 73 2025-06-30 14:30 to 2025-06-30 14:35\n",
      "Frame 74 2025-06-30 14:35 to 2025-06-30 14:40\n",
      "Frame 75 2025-06-30 14:40 to 2025-06-30 14:45\n",
      "Frame 76 2025-06-30 14:45 to 2025-06-30 14:50\n",
      "Frame 77 2025-06-30 14:50 to 2025-06-30 14:55\n",
      "Frame 78 2025-06-30 14:55 to 2025-06-30 15:00\n",
      "Frame 79 2025-06-30 15:00 to 2025-06-30 15:05\n",
      "Frame 80 2025-06-30 15:05 to 2025-06-30 15:10\n",
      "Frame 81 2025-06-30 15:10 to 2025-06-30 15:15\n",
      "Frame 82 2025-06-30 15:15 to 2025-06-30 15:20\n",
      "Frame 83 2025-06-30 15:20 to 2025-06-30 15:25\n",
      "Frame 84 2025-06-30 15:25 to 2025-06-30 15:30\n",
      "Frame 85 2025-06-30 15:30 to 2025-06-30 15:35\n",
      "Frame 86 2025-06-30 15:35 to 2025-06-30 15:40\n",
      "Frame 87 2025-06-30 15:40 to 2025-06-30 15:45\n",
      "Frame 88 2025-06-30 15:45 to 2025-06-30 15:50\n",
      "Frame 89 2025-06-30 15:50 to 2025-06-30 15:55\n",
      "Frame 90 2025-06-30 15:55 to 2025-06-30 16:00\n",
      "Frame 91 2025-06-30 16:00 to 2025-06-30 16:05\n",
      "Frame 92 2025-06-30 16:05 to 2025-06-30 16:10\n",
      "Frame 93 2025-06-30 16:10 to 2025-06-30 16:15\n",
      "Frame 94 2025-06-30 16:15 to 2025-06-30 16:20\n",
      "Frame 95 2025-06-30 16:20 to 2025-06-30 16:25\n",
      "Frame 96 2025-06-30 16:25 to 2025-06-30 16:30\n",
      "Frame 97 2025-06-30 16:30 to 2025-06-30 16:35\n",
      "Frame 98 2025-06-30 16:35 to 2025-06-30 16:40\n",
      "Frame 99 2025-06-30 16:40 to 2025-06-30 16:45\n",
      "Frame 100 2025-06-30 16:45 to 2025-06-30 16:50\n",
      "Frame 101 2025-06-30 16:50 to 2025-06-30 16:55\n",
      "Frame 102 2025-06-30 16:55 to 2025-06-30 17:00\n",
      "Frame 103 2025-06-30 17:00 to 2025-06-30 17:05\n",
      "Frame 104 2025-06-30 17:05 to 2025-06-30 17:10\n",
      "Frame 105 2025-06-30 17:10 to 2025-06-30 17:15\n",
      "Frame 106 2025-06-30 17:15 to 2025-06-30 17:20\n",
      "Frame 107 2025-06-30 17:20 to 2025-06-30 17:25\n",
      "Frame 108 2025-06-30 17:25 to 2025-06-30 17:30\n",
      "Frame 109 2025-06-30 17:30 to 2025-06-30 17:35\n",
      "Done\n",
      "Saved 109 adjacency matrices to a Pickle file.\n",
      "Saved 109 transmissions lists to a Pickle file.\n",
      "Saved 109 states to a Pickle file.\n"
     ]
    }
   ],
   "source": [
    "# Generate the state of all nodes in G for each frame of the animation\n",
    "\n",
    "t = tmin\n",
    "frame = 0\n",
    "\n",
    "tstate = None\n",
    "print('Calculating the network for each frame of the sim...')\n",
    "nodes0 = list(G.nodes()) # We only look at the nodes we already selected before (which have enough interactions over the entire period of the sim)\n",
    "daily_adj = np.zeros((num_nat_intervals, len(nodes0), len(nodes0)))\n",
    "daily_inf = []\n",
    "daily_states = []\n",
    "\n",
    "# Versions of daily_inf and daily_states using indexing reflecting number of nodes after filtering\n",
    "daily_inf_idx0 = []\n",
    "daily_states_idx0 = []\n",
    "# Some notes to explain the need for these additional vars: \n",
    "# In principle, each participant gets a UUID as well as a unique numerical ID, e.g: 9157781f-a054-424b-aa23-4e85ba4e8541 (UUID) \n",
    "# and 10310 (user numerical ID). One can use the dictionaries idTop2p and p2pToId to resolve the mappings between user ID and UUIDs.\n",
    "# Then, each user was assigned a numerical index from 0 to N-1, where N is the number of participants in a particular sim. \n",
    "# So for example, user with ID 10310 is 0, and user with ID 10311 is 1, and so on.\n",
    "# However, as some users can be filtered out for the network analyses if they do not have any contacts, this potentially reduces the\n",
    "# number of nodes in the network down to M < N. This is the size of the networkX object G. The thing is that the nodes G are labelled\n",
    "# with the indices from 0 to N-1, not 0 to M-1, and as it turns out, the values in the daily_states array correspond to the later, \n",
    "# not the former, so you you would need to convert the the values in daily_inf [0 to N-1] to the [0, M-1] range. Fortunately, one can \n",
    "# do this using the nodes0 array, which is simply the list of node labels in the G object. \n",
    "\n",
    "while t <= tmax:\n",
    "    t0 = t\n",
    "    t += nat_time_delta_sec\n",
    "    td = datetime.fromtimestamp(t, tz=timezone)\n",
    "    print('Frame', frame+1, datetime.fromtimestamp(t0, tz=timezone).strftime('%Y-%m-%d %H:%M'), 'to', td.strftime('%Y-%m-%d %H:%M'))\n",
    "    \n",
    "    # We want to include contact and infection events that either started or ended between t0 and t\n",
    "    condition = ((t0 < events['event_start']) & (events['event_start'] <= t)) | ((t0 < events['time']) & (events['time'] <= t))\n",
    "    tevents = events[condition]\n",
    "    tstate = get_node_state(user_index, tevents, tstate, p2pToId, data_format, False)\n",
    "    tinf = get_infection_list(user_index, tevents, discard_reinfections, nat_time_delta_sec, p2pToId, data_format, False)\n",
    "    tcontacts = get_contact_list(user_index, tevents, tinf, def_contact_time, False)\n",
    "    \n",
    "    tg = nx.Graph()\n",
    "    tg.add_nodes_from(nodes0)\n",
    "    tedges = []\n",
    "    tweights = []\n",
    "    if 0 < len(tcontacts):\n",
    "        for p in tcontacts:\n",
    "            n0 = p[0]\n",
    "            n1 = p[1]\n",
    "            w = tcontacts[p]            \n",
    "            if n0 in nodes0 and n1 in nodes0 and 0 < w:\n",
    "                tedges += [(n0, n1)]\n",
    "                tweights += [w]\n",
    "\n",
    "\n",
    "    daily_states.append([tstate[idx] for idx in list(tg.nodes())])    \n",
    "    daily_inf.append(tinf)\n",
    "\n",
    "    # [1,M-1] indexed variables:\n",
    "    tinf_idx0 = []\n",
    "    for i, j in tinf:\n",
    "        tinf_idx0.append((nodes0.index(i), nodes0.index(j)))\n",
    "    daily_inf_idx0.append(tinf_idx0)\n",
    "    daily_states_idx0.append([daily_states[frame][nodes0.index(idx)] for idx in list(tg.nodes())])    \n",
    "    \n",
    "    tg.add_weighted_edges_from([(tedges[i][0], tedges[i][1], tweights[i]) for i in range(len(tedges))])\n",
    "    adjm = nx.adjacency_matrix(tg).todense()\n",
    "    daily_adj[frame, :, :] = adjm\n",
    "    \n",
    "    frame += 1\n",
    "print('Done')\n",
    "num_frames = frame\n",
    "\n",
    "np.save(path.join(data_folder, 'daily-contact-matrices.npy'), daily_adj)\n",
    "print(f'Saved {frame} adjacency matrices to a Pickle file.')\n",
    "\n",
    "with open(path.join(data_folder, 'daily-transmissions.npy'), 'wb') as f:\n",
    "    pickle.dump(daily_inf, f)\n",
    "\n",
    "with open(path.join(data_folder, 'daily-transmissions-idx0.npy'), 'wb') as f:\n",
    "    pickle.dump(daily_inf_idx0, f)\n",
    "\n",
    "print(f'Saved {frame} transmissions lists to a Pickle file.')    \n",
    "\n",
    "with open(path.join(data_folder, 'daily-node-states.pickle'), 'wb') as f:\n",
    "    pickle.dump(daily_states, f)\n",
    "\n",
    "with open(path.join(data_folder, 'daily-node-states-idx0.pickle'), 'wb') as f:\n",
    "    pickle.dump(daily_states_idx0, f)\n",
    "\n",
    "print(f'Saved {frame} states to a Pickle file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ece9676a-b6c0-4e41-991a-04025fb90ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating score-producing events for each frame of the sim...\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "2025-06-30 0 0 0 0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "dates = []\n",
    "masking = []\n",
    "medication = []\n",
    "quarantine_no = []\n",
    "quarantine_yes = []\n",
    "\n",
    "t = tmin\n",
    "frame = 0\n",
    "print('Calculating score-producing events for each frame of the sim...')\n",
    "while t <= tmax:\n",
    "    t0 = t\n",
    "    t += nat_time_delta_sec\n",
    "    td = datetime.fromtimestamp(t, tz=timezone)\n",
    "    \n",
    "    # We want to include events that ocurred between t0 and t\n",
    "    condition = (t0 < events['time']) & (events['time'] <= t)\n",
    "\n",
    "    tevents = events[condition]    \n",
    "    score_events = get_score_events(tevents)\n",
    "\n",
    "    date = datetime.fromtimestamp(t0, tz=timezone).strftime('%Y-%m-%d')\n",
    "    mask = get_info_count(score_events, \"shopMask\")\n",
    "    med = get_info_count(score_events, \"shopMedication\")\n",
    "    qno = get_info_count(score_events, \"noQuarantine\")\n",
    "    qyes = get_info_count(score_events, \"quarantine\")\n",
    "\n",
    "    print(date, mask, med, qno, qyes) \n",
    "\n",
    "    dates.append(date)\n",
    "    masking.append(mask)\n",
    "    medication.append(med)\n",
    "    quarantine_no.append(qno)\n",
    "    quarantine_yes.append(qyes)\n",
    "    \n",
    "    frame += 1\n",
    "print('Done')\n",
    "\n",
    "daily_behaviors = pd.DataFrame({'date': dates, \n",
    "                                'masking': masking, \n",
    "                                'medication': medication, \n",
    "                                'quarantine_no': quarantine_no, \n",
    "                                'quarantine_yes': quarantine_yes})\n",
    "\n",
    "with open(path.join(data_folder, 'daily-behaviors.pickle'), 'wb') as f:\n",
    "    pickle.dump(daily_behaviors, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029ed192-28f3-4afa-86bc-04dd60639490",
   "metadata": {},
   "source": [
    "## EXTRAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d264f229-e762-4854-974d-c20163a81934",
   "metadata": {},
   "source": [
    "## Converting data to R-compatible format\n",
    "\n",
    "Only needed if planning to conduct difussion analysis in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3fed6542-ecdf-4486-bf1a-2c05735948cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting the network for each frame of the sim...\n",
      "Frame 1 2025-06-30 08:30 to 2025-06-30 08:35\n",
      "Frame 2 2025-06-30 08:35 to 2025-06-30 08:40\n",
      "Frame 3 2025-06-30 08:40 to 2025-06-30 08:45\n",
      "Frame 4 2025-06-30 08:45 to 2025-06-30 08:50\n",
      "Frame 5 2025-06-30 08:50 to 2025-06-30 08:55\n",
      "Frame 6 2025-06-30 08:55 to 2025-06-30 09:00\n",
      "Frame 7 2025-06-30 09:00 to 2025-06-30 09:05\n",
      "Frame 8 2025-06-30 09:05 to 2025-06-30 09:10\n",
      "Frame 9 2025-06-30 09:10 to 2025-06-30 09:15\n",
      "Frame 10 2025-06-30 09:15 to 2025-06-30 09:20\n",
      "Frame 11 2025-06-30 09:20 to 2025-06-30 09:25\n",
      "Frame 12 2025-06-30 09:25 to 2025-06-30 09:30\n",
      "Frame 13 2025-06-30 09:30 to 2025-06-30 09:35\n",
      "Frame 14 2025-06-30 09:35 to 2025-06-30 09:40\n",
      "Frame 15 2025-06-30 09:40 to 2025-06-30 09:45\n",
      "Frame 16 2025-06-30 09:45 to 2025-06-30 09:50\n",
      "Frame 17 2025-06-30 09:50 to 2025-06-30 09:55\n",
      "Frame 18 2025-06-30 09:55 to 2025-06-30 10:00\n",
      "Frame 19 2025-06-30 10:00 to 2025-06-30 10:05\n",
      "Frame 20 2025-06-30 10:05 to 2025-06-30 10:10\n",
      "Frame 21 2025-06-30 10:10 to 2025-06-30 10:15\n",
      "Frame 22 2025-06-30 10:15 to 2025-06-30 10:20\n",
      "Frame 23 2025-06-30 10:20 to 2025-06-30 10:25\n",
      "Frame 24 2025-06-30 10:25 to 2025-06-30 10:30\n",
      "Frame 25 2025-06-30 10:30 to 2025-06-30 10:35\n",
      "Frame 26 2025-06-30 10:35 to 2025-06-30 10:40\n",
      "Frame 27 2025-06-30 10:40 to 2025-06-30 10:45\n",
      "Frame 28 2025-06-30 10:45 to 2025-06-30 10:50\n",
      "Frame 29 2025-06-30 10:50 to 2025-06-30 10:55\n",
      "Frame 30 2025-06-30 10:55 to 2025-06-30 11:00\n",
      "Frame 31 2025-06-30 11:00 to 2025-06-30 11:05\n",
      "Frame 32 2025-06-30 11:05 to 2025-06-30 11:10\n",
      "Frame 33 2025-06-30 11:10 to 2025-06-30 11:15\n",
      "Frame 34 2025-06-30 11:15 to 2025-06-30 11:20\n",
      "Frame 35 2025-06-30 11:20 to 2025-06-30 11:25\n",
      "Frame 36 2025-06-30 11:25 to 2025-06-30 11:30\n",
      "Frame 37 2025-06-30 11:30 to 2025-06-30 11:35\n",
      "Frame 38 2025-06-30 11:35 to 2025-06-30 11:40\n",
      "Frame 39 2025-06-30 11:40 to 2025-06-30 11:45\n",
      "Frame 40 2025-06-30 11:45 to 2025-06-30 11:50\n",
      "Frame 41 2025-06-30 11:50 to 2025-06-30 11:55\n",
      "Frame 42 2025-06-30 11:55 to 2025-06-30 12:00\n",
      "Frame 43 2025-06-30 12:00 to 2025-06-30 12:05\n",
      "Frame 44 2025-06-30 12:05 to 2025-06-30 12:10\n",
      "Frame 45 2025-06-30 12:10 to 2025-06-30 12:15\n",
      "Frame 46 2025-06-30 12:15 to 2025-06-30 12:20\n",
      "Frame 47 2025-06-30 12:20 to 2025-06-30 12:25\n",
      "Frame 48 2025-06-30 12:25 to 2025-06-30 12:30\n",
      "Frame 49 2025-06-30 12:30 to 2025-06-30 12:35\n",
      "Frame 50 2025-06-30 12:35 to 2025-06-30 12:40\n",
      "Frame 51 2025-06-30 12:40 to 2025-06-30 12:45\n",
      "Frame 52 2025-06-30 12:45 to 2025-06-30 12:50\n",
      "Frame 53 2025-06-30 12:50 to 2025-06-30 12:55\n",
      "Frame 54 2025-06-30 12:55 to 2025-06-30 13:00\n",
      "Frame 55 2025-06-30 13:00 to 2025-06-30 13:05\n",
      "Frame 56 2025-06-30 13:05 to 2025-06-30 13:10\n",
      "Frame 57 2025-06-30 13:10 to 2025-06-30 13:15\n",
      "Frame 58 2025-06-30 13:15 to 2025-06-30 13:20\n",
      "Frame 59 2025-06-30 13:20 to 2025-06-30 13:25\n",
      "Frame 60 2025-06-30 13:25 to 2025-06-30 13:30\n",
      "Frame 61 2025-06-30 13:30 to 2025-06-30 13:35\n",
      "Frame 62 2025-06-30 13:35 to 2025-06-30 13:40\n",
      "Frame 63 2025-06-30 13:40 to 2025-06-30 13:45\n",
      "Frame 64 2025-06-30 13:45 to 2025-06-30 13:50\n",
      "Frame 65 2025-06-30 13:50 to 2025-06-30 13:55\n",
      "Frame 66 2025-06-30 13:55 to 2025-06-30 14:00\n",
      "Frame 67 2025-06-30 14:00 to 2025-06-30 14:05\n",
      "Frame 68 2025-06-30 14:05 to 2025-06-30 14:10\n",
      "Frame 69 2025-06-30 14:10 to 2025-06-30 14:15\n",
      "Frame 70 2025-06-30 14:15 to 2025-06-30 14:20\n",
      "Frame 71 2025-06-30 14:20 to 2025-06-30 14:25\n",
      "Frame 72 2025-06-30 14:25 to 2025-06-30 14:30\n",
      "Frame 73 2025-06-30 14:30 to 2025-06-30 14:35\n",
      "Frame 74 2025-06-30 14:35 to 2025-06-30 14:40\n",
      "Frame 75 2025-06-30 14:40 to 2025-06-30 14:45\n",
      "Frame 76 2025-06-30 14:45 to 2025-06-30 14:50\n",
      "Frame 77 2025-06-30 14:50 to 2025-06-30 14:55\n",
      "Frame 78 2025-06-30 14:55 to 2025-06-30 15:00\n",
      "Frame 79 2025-06-30 15:00 to 2025-06-30 15:05\n",
      "Frame 80 2025-06-30 15:05 to 2025-06-30 15:10\n",
      "Frame 81 2025-06-30 15:10 to 2025-06-30 15:15\n",
      "Frame 82 2025-06-30 15:15 to 2025-06-30 15:20\n",
      "Frame 83 2025-06-30 15:20 to 2025-06-30 15:25\n",
      "Frame 84 2025-06-30 15:25 to 2025-06-30 15:30\n",
      "Frame 85 2025-06-30 15:30 to 2025-06-30 15:35\n",
      "Frame 86 2025-06-30 15:35 to 2025-06-30 15:40\n",
      "Frame 87 2025-06-30 15:40 to 2025-06-30 15:45\n",
      "Frame 88 2025-06-30 15:45 to 2025-06-30 15:50\n",
      "Frame 89 2025-06-30 15:50 to 2025-06-30 15:55\n",
      "Frame 90 2025-06-30 15:55 to 2025-06-30 16:00\n",
      "Frame 91 2025-06-30 16:00 to 2025-06-30 16:05\n",
      "Frame 92 2025-06-30 16:05 to 2025-06-30 16:10\n",
      "Frame 93 2025-06-30 16:10 to 2025-06-30 16:15\n",
      "Frame 94 2025-06-30 16:15 to 2025-06-30 16:20\n",
      "Frame 95 2025-06-30 16:20 to 2025-06-30 16:25\n",
      "Frame 96 2025-06-30 16:25 to 2025-06-30 16:30\n",
      "Frame 97 2025-06-30 16:30 to 2025-06-30 16:35\n",
      "Frame 98 2025-06-30 16:35 to 2025-06-30 16:40\n",
      "Frame 99 2025-06-30 16:40 to 2025-06-30 16:45\n",
      "Frame 100 2025-06-30 16:45 to 2025-06-30 16:50\n",
      "Frame 101 2025-06-30 16:50 to 2025-06-30 16:55\n",
      "Frame 102 2025-06-30 16:55 to 2025-06-30 17:00\n",
      "Frame 103 2025-06-30 17:00 to 2025-06-30 17:05\n",
      "Frame 104 2025-06-30 17:05 to 2025-06-30 17:10\n",
      "Frame 105 2025-06-30 17:10 to 2025-06-30 17:15\n",
      "Frame 106 2025-06-30 17:15 to 2025-06-30 17:20\n",
      "Frame 107 2025-06-30 17:20 to 2025-06-30 17:25\n",
      "Frame 108 2025-06-30 17:25 to 2025-06-30 17:30\n",
      "Frame 109 2025-06-30 17:30 to 2025-06-30 17:35\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Exporting the network for each frame of the sim...')\n",
    "\n",
    "tstate = None\n",
    "\n",
    "nodes0 = list(G.nodes()) # We only look at the nodes we already selected before (which have enough interactions over the entire period of the sim)\n",
    "daily_adj = np.zeros((15, len(nodes0), len(nodes0)))\n",
    "daily_inf = []\n",
    "daily_states = []\n",
    "\n",
    "N = len(nodes0)\n",
    "toa = np.array([10000] * N)\n",
    "qyes_list = []\n",
    "qno_list = []\n",
    "mask_list = []\n",
    "med_list = []\n",
    "\n",
    "t = tmin\n",
    "frame = 0\n",
    "while t <= tmax:\n",
    "    t0 = t\n",
    "    t += nat_time_delta_sec\n",
    "    td = datetime.fromtimestamp(t, tz=timezone)\n",
    "    print('Frame', frame+1, datetime.fromtimestamp(t0, tz=timezone).strftime('%Y-%m-%d %H:%M'), 'to', td.strftime('%Y-%m-%d %H:%M'))\n",
    "    \n",
    "    # We want to include contact and infection events that either started or ended between t0 and t\n",
    "    condition = ((t0 < events['event_start']) & (events['event_start'] <= t)) | ((t0 < events['time']) & (events['time'] <= t))\n",
    "    tevents = events[condition]\n",
    "    tstate = get_node_state(user_index, tevents, tstate, p2pToId, data_format, False)\n",
    "    tinf = get_infection_list(user_index, tevents, discard_reinfections, nat_time_delta_sec, p2pToId, data_format, False)\n",
    "    tcontacts = get_contact_list(user_index, tevents, tinf, def_contact_time, p2pToId, data_format, False)\n",
    "    score_events = get_score_events(tevents)\n",
    "    \n",
    "    tg = nx.Graph()\n",
    "    tg.add_nodes_from(nodes0)\n",
    "    tedges = []\n",
    "    tweights = []\n",
    "    if 0 < len(tcontacts):\n",
    "        for p in tcontacts:\n",
    "            n0 = p[0]\n",
    "            n1 = p[1]\n",
    "            w = tcontacts[p]            \n",
    "            if n0 in nodes0 and n1 in nodes0 and 0 < w:\n",
    "                tedges += [(n0, n1)]\n",
    "                tweights += [w]\n",
    "\n",
    "    tg.add_weighted_edges_from([(tedges[i][0], tedges[i][1], tweights[i]) for i in range(len(tedges))])\n",
    "    m = nx.to_scipy_sparse_array(tg, format=\"csr\")\n",
    "    mmwrite(f\"{data_folder}/network_{frame+1}.mtx\", m)\n",
    "    \n",
    "    fstate = np.array([tstate[idx] for idx in list(tg.nodes())])\n",
    "    inf_id = np.where((fstate == 1) | (fstate == 2))[0]\n",
    "    mask = frame + 1 < toa[inf_id]\n",
    "    toa[inf_id[mask]] = frame + 1\n",
    "\n",
    "    qyes = np.zeros(N)    \n",
    "    users = get_info_users(score_events, \"quarantine\")    \n",
    "    nidx = get_node_index(users, user_index, nodes0)\n",
    "    qyes[nidx] = 1\n",
    "    qyes_list.append(qyes)\n",
    "    \n",
    "    qno = np.zeros(N)\n",
    "    users = get_info_users(score_events, \"noQuarantine\")\n",
    "    nidx = get_node_index(users, user_index, nodes0)\n",
    "    qno[nidx] = 1\n",
    "    qno_list.append(qno)\n",
    "\n",
    "    mask = np.zeros(N)\n",
    "    users = get_info_users(score_events, \"shopMask\")\n",
    "    nidx = get_node_index(users, user_index, nodes0)\n",
    "    mask[nidx] = 1 \n",
    "    mask_list.append(mask)\n",
    "\n",
    "    med = np.zeros(N)\n",
    "    users = get_info_users(score_events, \"shopMedication\")\n",
    "    nidx = get_node_index(users, user_index, nodes0)\n",
    "    med[nidx] = 1\n",
    "    med_list.append(med)\n",
    "    \n",
    "    frame += 1\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0bc1dea8-c3f3-4b6d-9fd1-f2c4b9590a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qyes_all = np.concatenate(qyes_list)\n",
    "qno_all = np.concatenate(qno_list)\n",
    "mask_all = np.concatenate(mask_list)\n",
    "med_all = np.concatenate(med_list)\n",
    "\n",
    "toa = toa.astype(float)\n",
    "toa[toa == 10000] = np.nan\n",
    "toa_all = np.tile(toa, num_frames)\n",
    "\n",
    "nodes_all = np.tile(np.arange(N) + 1, num_frames)\n",
    "\n",
    "t_all = np.repeat(np.arange(1, num_frames + 1), N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fcfb0d64-86bc-4811-bab9-fb6e1012f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dat = pd.DataFrame({'idnum': nodes_all, 'qyes': qyes_all, 'qno': qno_all, 'mask': mask_all, 'med': med_all, 'tinf': toa_all, 'time': t_all})\n",
    "net_dat['tinf'] = net_dat['tinf'].astype('Int64')\n",
    "net_dat.to_csv(f\"{data_folder}/network_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab5542-3713-4d64-91c1-8c9430409528",
   "metadata": {},
   "source": [
    "## Exporting informaton of infected nodes\n",
    "\n",
    "Used during PSI conference to generate input data for GLEAMviz sim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40714150-a444-4994-9f42-8a14586df33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tinf_step_min = 60\n",
    "tinf_delta_sec = 60 * tinf_step_min\n",
    "\n",
    "max_frame = 4\n",
    "\n",
    "t = tmin\n",
    "frame = 0\n",
    "\n",
    "cases = []\n",
    "\n",
    "tstate = None\n",
    "print('Calculating transmissions and case info...')\n",
    "nodes0 = list(G.nodes()) # We only look at the nodes we already selected before (which have enough interactions over the entire period of the sim)\n",
    "daily_adj = np.zeros((num_frames, len(nodes0), len(nodes0)))\n",
    "daily_inf = []\n",
    "daily_states = []\n",
    "while t <= tmax and frame < max_frame:\n",
    "    t0 = t\n",
    "    t += tinf_delta_sec\n",
    "    td = datetime.fromtimestamp(t, tz=timezone)\n",
    "    print('Frame', frame+1, datetime.fromtimestamp(t0, tz=timezone).strftime('%Y-%m-%d %H:%M'), 'to', td.strftime('%Y-%m-%d %H:%M'))\n",
    "    \n",
    "    # We want to include contact and infection events that either started or ended between t0 and t\n",
    "    condition = ((t0 < events['event_start']) & (events['event_start'] <= t)) | ((t0 < events['time']) & (events['time'] <= t))\n",
    "    tevents = events[condition]\n",
    "    tstate = get_node_state(user_index, tevents, tstate, False)\n",
    "    tinf = get_infection_list(user_index, tevents, discard_reinfections, time_delta_sec)\n",
    "\n",
    "    for pair in tinf:\n",
    "        cases.append(index_user[pair[1]])\n",
    "\n",
    "    frame += 1\n",
    "print('Done')\n",
    "\n",
    "countries = all_events[all_events['type'] == 'modifier']\n",
    "inf_ids = []\n",
    "inf_countries = []\n",
    "for cid in cases:\n",
    "    sel = countries[countries['user_id'] == cid]\n",
    "    country = sel['modifier'].values[0].split(':')[1]\n",
    "    inf_ids.append(cid)\n",
    "    inf_countries.append(country)\n",
    "\n",
    "inf_data = pd.DataFrame({\n",
    "    \"ID\": inf_ids,\n",
    "    \"Country\": inf_countries,\n",
    "    \"State\": ['infectious'] * len(inf_ids)\n",
    "})\n",
    "\n",
    "print(inf_data)\n",
    "\n",
    "inf_data.to_csv(path.join(output_folder, 'gleam-input.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
